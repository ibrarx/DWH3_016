{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f7d15c",
   "metadata": {},
   "source": [
    "First, create a new conda environment named BI2025 and install the required packages from requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2329db9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting starvers@ git+https://github.com/AllStarsAT/starvers.git (from -r requirements.txt (line 4))\n",
      "  Cloning https://github.com/AllStarsAT/starvers.git to c:\\users\\ibrar\\appdata\\local\\temp\\pip-install-tvbhsyd3\\starvers_276e6140e53b4a8186f618d459c79f2d\n",
      "  Resolved https://github.com/AllStarsAT/starvers.git to commit 5e6e112e2b37cb5a27af9585bd572d27187ef735\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: ipykernel in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from -r requirements.txt (line 1)) (7.1.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from -r requirements.txt (line 2)) (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from -r requirements.txt (line 3)) (3.10.7)\n",
      "Requirement already satisfied: plotly in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from -r requirements.txt (line 5)) (6.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from -r requirements.txt (line 6)) (2.32.5)\n",
      "Requirement already satisfied: pandas==2.3.3 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (2.3.3)\n",
      "Requirement already satisfied: pytest==7.1.3 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (7.1.3)\n",
      "Requirement already satisfied: rdflib==6.2.0 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (6.2.0)\n",
      "Requirement already satisfied: setuptools==65.4.0 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (65.4.0)\n",
      "Requirement already satisfied: SPARQLWrapper==2.0.0 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (2.0.0)\n",
      "Requirement already satisfied: tzlocal==4.2 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (4.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from pandas==2.3.3->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from pandas==2.3.3->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from pandas==2.3.3->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from pandas==2.3.3->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from pytest==7.1.3->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (25.4.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from pytest==7.1.3->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from pytest==7.1.3->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from pytest==7.1.3->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (1.6.0)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from pytest==7.1.3->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from pytest==7.1.3->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from pytest==7.1.3->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: isodate in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from rdflib==6.2.0->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (0.7.2)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from rdflib==6.2.0->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (3.2.5)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from tzlocal==4.2->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (0.1.0.post0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipykernel->-r requirements.txt (line 1)) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipykernel->-r requirements.txt (line 1)) (1.8.17)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipykernel->-r requirements.txt (line 1)) (9.8.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipykernel->-r requirements.txt (line 1)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipykernel->-r requirements.txt (line 1)) (5.9.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipykernel->-r requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipykernel->-r requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipykernel->-r requirements.txt (line 1)) (7.1.3)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipykernel->-r requirements.txt (line 1)) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipykernel->-r requirements.txt (line 1)) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipykernel->-r requirements.txt (line 1)) (5.14.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (12.0.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from plotly->-r requirements.txt (line 5)) (2.13.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (2025.11.12)\n",
      "Requirement already satisfied: decorator>=4.3.2 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.8.5)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 1)) (4.5.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.3.3->starvers@ git+https://github.com/AllStarsAT/starvers.git->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\ibrar\\anaconda3\\envs\\bi2025\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/AllStarsAT/starvers.git 'C:\\Users\\ibrar\\AppData\\Local\\Temp\\pip-install-tvbhsyd3\\starvers_276e6140e53b4a8186f618d459c79f2d'\n"
     ]
    }
   ],
   "source": [
    "# !conda create -n BI2025 python=3.11 -y\n",
    "# !conda activate BI2025\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5122654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY OR COPY THIS CELL!! \n",
    "# Note: The only imports allowed are Python's standard library, pandas, numpy, scipy, matplotlib, seaborn and scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import typing\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "from starvers.starvers import TripleStoreEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79408d3",
   "metadata": {},
   "source": [
    "## Graph-based documentation preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831a95c",
   "metadata": {},
   "source": [
    "**!!!IMPORTANT!!!**\n",
    "\n",
    "Everytime you work on this notebook, enter your student ID in the `executed_by` variable so that the cell executions are accredited to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a02423",
   "metadata": {},
   "outputs": [],
   "source": [
    "executed_by ='stud-id_12350094'  # Replace the digits after \"id_\" with your own student ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2160a7",
   "metadata": {},
   "source": [
    "Set your group and student IDs. Do this only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16721334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group id for this project\n",
    "group_id = '16'  # Replace the digits with your group id\n",
    "\n",
    "# Students working on this notebook\n",
    "student_a = 'stud-id_12350094'  # Replace the digits after \"id_\" with student A's student ID\n",
    "student_b = 'stud-id_11826186'  # Replace the digits after \"id_\" with student B's student ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb927186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roles. Don't change these values.\n",
    "code_writer_role = 'code_writer'\n",
    "code_executor_role = 'code_executor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e253f6",
   "metadata": {},
   "source": [
    "Setup the starvers API for logging your steps into our server-sided graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4195fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025\"\n",
    "post_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025/statements\"\n",
    "engine = TripleStoreEngine(get_endpoint, post_endpoint, skip_connection_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cee91",
   "metadata": {},
   "source": [
    "Use these prefixes in your notebooks. You can extend this dict with your prefixes of additional ontologies that you use in this notebook. Replace 00 with your group id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e6f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = {\n",
    "    'xsd': 'http://www.w3.org/2001/XMLSchema#',\n",
    "    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "    'foaf': 'http://xmlns.com/foaf/0.1/',\n",
    "    'prov': 'http://www.w3.org/ns/prov#',\n",
    "    'sc': 'https://schema.org/',\n",
    "    'cr': 'http://mlcommons.org/croissant/',\n",
    "    'mls': 'http://www.w3.org/ns/mls#',\n",
    "    'mlso': 'http://w3id.org/mlso',\n",
    "    'siu': 'https://si-digital-framework.org/SI/units/',\n",
    "    'siq': 'https://si-digital-framework.org/SI/quantities/',\n",
    "    'qudt': 'http://qudt.org/schema/qudt/',\n",
    "    '': f'https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/',\n",
    "}\n",
    "\n",
    "prefix_header = '\\n'.join([f'PREFIX {k}: <{v}>' for k, v in prefixes.items()]) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970468d",
   "metadata": {},
   "source": [
    "Ontologies to use\n",
    "* Provenance of the experiment process\n",
    "    * PROV-O: \n",
    "        * doc: https://www.w3.org/TR/prov-o/\n",
    "        * serialization: https://www.w3.org/ns/prov-o\n",
    "* Data used and created\n",
    "    * schema.org - Dataset: \n",
    "        * doc: https://schema.org/Dataset\n",
    "        * serialization: https://schema.org/version/latest/schemaorg-current-https.ttl\n",
    "    * Crossaint\n",
    "        * doc: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n",
    "        * serialization: https://github.com/mlcommons/croissant/blob/main/docs/croissant.ttl\n",
    "* ML experiments performed\n",
    "    * MLSO: \n",
    "        * doc: https://github.com/dtai-kg/MLSO\n",
    "        * doc: https://dtai-kg.github.io/MLSO/#http://w3id.org/\n",
    "        * serialization: https://dtai-kg.github.io/MLSO/ontology.ttl\n",
    "* Measurements, Metrics, Units\n",
    "    * QUDT\n",
    "        * doc:https://qudt.org/\n",
    "        * doc: https://github.com/qudt/qudt-public-repo\n",
    "        * serialization: https://github.com/qudt/qudt-public-repo/blob/main/src/main/rdf/schema/SCHEMA_QUDT.ttl\n",
    "    * SI Digital Framework\n",
    "        * doc: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/docs/README.md\n",
    "        * doc: https://si-digital-framework.org/\n",
    "        * doc: https://si-digital-framework.org/SI\n",
    "        * serialization: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/TTL/si.ttl\n",
    "    * Quantities and Units\n",
    "        * doc: https://www.omg.org/spec/Commons\n",
    "        * serialization: https://www.omg.org/spec/Commons/QuantitiesAndUnits.ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62393d",
   "metadata": {},
   "source": [
    "Use this function to record execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f08ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time in ISO 8601 format with UTC timezone in the following format:\n",
    "    YYYY-MM-DDTHH:MM:SS.sssZ\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    timestamp_formated = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]  +\"Z\"\n",
    "\n",
    "    return timestamp_formated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a1605",
   "metadata": {},
   "source": [
    "Register yourself in the Knowledge Graph using ProvO. Change the given name, family name and immatriculation number to reflect your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4080a558",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 504: Gateway Time-out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     13\u001b[39m reigstration_triples_b = [\n\u001b[32m     14\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type foaf:Person .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Agent .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m <http://purl.obolibrary.org/obo/IAO_0000219> \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m76543210\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m^^xsd:string .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     22\u001b[39m ]\n\u001b[32m     24\u001b[39m role_triples = [\n\u001b[32m     25\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_writer_role\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Role .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     26\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_executor_role\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Role .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     27\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreigstration_triples_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m engine.insert(reigstration_triples_b, prefixes=prefixes)\n\u001b[32m     32\u001b[39m engine.insert(role_triples, prefixes=prefixes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\starvers\\starvers.py:512\u001b[39m, in \u001b[36mTripleStoreEngine.insert\u001b[39m\u001b[34m(self, triples, prefixes, timestamp, chunk_size)\u001b[39m\n\u001b[32m    510\u001b[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001b[33m\"\u001b[39m\u001b[33mNOW()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mself\u001b[39m.sparql_post.setQuery(insert_statement)\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparql_post\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mTriples inserted.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:960\u001b[39m, in \u001b[36mSPARQLWrapper.query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mQueryResult\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    943\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m    Execute the query.\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m \u001b[33;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:940\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EndPointInternalError(e.read())\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:926\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    924\u001b[39m         response = urlopener(request, timeout=\u001b[38;5;28mself\u001b[39m.timeout)\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         response = \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m.returnFormat\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:216\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    215\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:525\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    524\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:634\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:563\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    562\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:496\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    495\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    498\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:643\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 504: Gateway Time-out"
     ]
    }
   ],
   "source": [
    "# Ontologies used: foaf, prov, IAO\n",
    "reigstration_triples_a = [\n",
    "f':{student_a} rdf:type foaf:Person .',\n",
    "f':{student_a} rdf:type prov:Agent .',\n",
    "f':{student_a} foaf:givenName \"Muhammad\" .',\n",
    "f':{student_a} foaf:familyName \"Ibrar\" .',\n",
    "f':{student_a} <http://vivoweb.org/ontology/core#identifier> :{student_a} .',\n",
    "f':{student_a} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_a} <http://www.w3.org/2000/01/rdf-schema#label> \"12350094\" .',\n",
    "f':{student_a} <http://purl.obolibrary.org/obo/IAO_0000219> \"01234567\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "reigstration_triples_b = [\n",
    "f':{student_b} rdf:type foaf:Person .',\n",
    "f':{student_b} rdf:type prov:Agent .',\n",
    "f':{student_b} foaf:givenName \"Ahmad\" .',\n",
    "f':{student_b} foaf:familyName \"Ibrahim\" .',\n",
    "f':{student_b} <http://vivoweb.org/ontology/core#identifier> :{student_b} .',\n",
    "f':{student_b} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_b} <http://www.w3.org/2000/01/rdf-schema#label> \"11826186\" .',\n",
    "f':{student_b} <http://purl.obolibrary.org/obo/IAO_0000219> \"76543210\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "role_triples = [\n",
    "    f':{code_writer_role} rdf:type prov:Role .',\n",
    "    f':{code_executor_role} rdf:type prov:Role .',\n",
    "]\n",
    "\n",
    "\n",
    "engine.insert(reigstration_triples_a, prefixes=prefixes)\n",
    "engine.insert(reigstration_triples_b, prefixes=prefixes)\n",
    "engine.insert(role_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c479ed4",
   "metadata": {},
   "source": [
    "**What not do do**\n",
    "\n",
    "Do not use [blank nodes](https://www.w3.org/wiki/BlankNodes).\n",
    "\n",
    "PROV-O uses blank nodes to connect multiple elements with each other.\n",
    "Such blank nodes (such as _:association) should not be used.\n",
    "Instead, assign a fixed node ID such as\n",
    ":5119fcd7-b571-41e0-9464-a37c7be0f574 by generating them outside of the\n",
    "notebook.\n",
    "We suggest that, for each setting where such a blank node is needed to\n",
    "connect multiple elements, you create a unique hash (using uuid.uuid4())\n",
    "and keep this as hard-coded identifier for the blank node. The template\n",
    "notebook contains examples of this. Do *not* use these provided values,\n",
    "as otherwise, your provenance documentations will all be connected via\n",
    "these identifiers!\n",
    "Also, do not generate them dynamically in every cell execution, e.g. by\n",
    "using uuid.uuid4() in a cell. This would generate many new linking nodes\n",
    "for connecting the same elements.\n",
    "Compute one for each node (cell) where you need them and make sure to\n",
    "use the same one on each re-execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "890a782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_data_path = os.path.join(\"data\", \"datasets\", \"weather\")\n",
    "# cyclists_data_path = os.path.join(\"data\", \"datasets\", \"cyclists\")\n",
    "mobile_price_data_path = os.path.join(\"data\", \"datasets\", \"mobile_price\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee069d",
   "metadata": {},
   "source": [
    "## Business Understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee88389",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 504: Gateway Time-out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## Each Activity that follows is part of the Business Understanding Phase\u001b[39;00m\n\u001b[32m      3\u001b[39m business_understanding_phase_executor = [\n\u001b[32m      4\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:business_understanding_phase rdf:type prov:Activity .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:business_understanding_phase rdfs:label \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBusiness Understanding Phase\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m .\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;66;03m## Phase 1: Business Understanding\u001b[39;00m\n\u001b[32m      6\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbusiness_understanding_phase_executor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\starvers\\starvers.py:512\u001b[39m, in \u001b[36mTripleStoreEngine.insert\u001b[39m\u001b[34m(self, triples, prefixes, timestamp, chunk_size)\u001b[39m\n\u001b[32m    510\u001b[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001b[33m\"\u001b[39m\u001b[33mNOW()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mself\u001b[39m.sparql_post.setQuery(insert_statement)\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparql_post\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mTriples inserted.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:960\u001b[39m, in \u001b[36mSPARQLWrapper.query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mQueryResult\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    943\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m    Execute the query.\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m \u001b[33;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:940\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EndPointInternalError(e.read())\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:926\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    924\u001b[39m         response = urlopener(request, timeout=\u001b[38;5;28mself\u001b[39m.timeout)\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         response = \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m.returnFormat\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:216\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    215\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:525\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    524\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:634\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:563\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    562\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:496\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    495\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    498\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ibrar\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:643\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 504: Gateway Time-out"
     ]
    }
   ],
   "source": [
    "## Each Activity that follows is part of the Business Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':business_understanding_phase rdf:type prov:Activity .',\n",
    "f':business_understanding_phase rdfs:label \"Business Understanding Phase\" .', ## Phase 1: Business Understanding\n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc8a3a-708a-4992-a076-038c53338e89",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9bd9643d1e26a8dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "data_src_and_scenario_comment = \"\"\"\n",
    "Data Source:\n",
    "The dataset is the Kaggle “Mobile Price Classification” dataset containing 2,000 mobile phones. Each phone is described by 20 technical features (battery power, RAM, internal memory, camera specs, connectivity options, screen dimensions, etc.) and one target variable price_range with four classes (0 = low, 3 = very high).\n",
    "\n",
    "Scenario:\n",
    "A new mobile company wants to price upcoming phone models competitively against major brands. Instead of relying only on expert judgment, the company wants to analyze historical specifications of phones and their corresponding price ranges to support pricing decisions.\n",
    "\"\"\"\n",
    "\n",
    "business_objectives_comment = \"\"\"\n",
    "1. Support pricing decisions by predicting the most suitable price range for new phone models.\n",
    "2. Reduce manual effort and time required to estimate price categories.\n",
    "3. Improve product positioning in budget, mid-range, high-end, and flagship segments.\n",
    "4. Increase transparency on how technical features influence pricing decisions.\n",
    "\"\"\"\n",
    "\n",
    "business_success_criteria_comment = \"\"\"\n",
    "1. The ML system is regularly used by product and pricing teams.\n",
    "2. At least a 30% reduction in time needed for initial price-range estimation.\n",
    "3. Most new models (>80%) remain in the initially selected price band after launch.\n",
    "4. Pricing decisions become more consistent and data-driven across phone segments.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_goals_comment = \"\"\"\n",
    "1. Build a multi-class classifier predicting the price_range (0–3) from 20 phone features.\n",
    "2. Achieve robust accuracy on unseen data and generalize well to new configurations.\n",
    "3. Identify important features (e.g., RAM, pixel resolution) influencing the price range.\n",
    "4. Provide probability outputs to support uncertainty-aware pricing decisions.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_success_criteria_comment = \"\"\"\n",
    "1. Achieve ≥90% accuracy on the validation/test set.\n",
    "2. Macro F1-score ≥0.88 with no class having recall <0.80.\n",
    "3. Model performance remains stable across different random splits.\n",
    "4. Probabilities are reasonably calibrated for business decision use.\n",
    "\"\"\"\n",
    "\n",
    "ai_risk_aspects_comment = \"\"\"\n",
    "1. Misclassification may lead to wrong pricing decisions, affecting revenue or sales.\n",
    "2. The dataset may not reflect future devices; model drift is a risk.\n",
    "3. Over-reliance on the model could cause poor decisions without expert review.\n",
    "4. Although no personal data is used, systematic bias across device categories is possible.\n",
    "5. Pricing logic leakage is a business security risk if the model is exposed externally.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "bu_ass_uuid_executor = \"79fd346c-c772-4e8c-92d8-583c5a1666ad\" # Generate once\n",
    "business_understanding_executor = [\n",
    "f':business_understanding rdf:type prov:Activity .',\n",
    "f':business_understanding sc:isPartOf :business_understanding_phase .', # Connect Activity to Parent Business Understanding Phase Activity\n",
    "f':business_understanding prov:qualifiedAssociation :{bu_ass_uuid_executor} .',\n",
    "f':{bu_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{bu_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{bu_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(business_understanding_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "business_understanding_data_executor = [\n",
    "# 1a\n",
    "f':bu_data_source_and_scenario rdf:type prov:Entity .',\n",
    "f':bu_data_source_and_scenario prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_source_and_scenario rdfs:label \"1a Data Source and Scenario\" .',\n",
    "f':bu_data_source_and_scenario rdfs:comment \"\"\"{data_src_and_scenario_comment}\"\"\" .',\n",
    "# 1b\n",
    "f':bu_business_objectives rdf:type prov:Entity .',\n",
    "f':bu_business_objectives prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_objectives rdfs:label \"1b Business Objectives\" .',\n",
    "f':bu_business_objectives rdfs:comment \"\"\"{business_objectives_comment}\"\"\" .',\n",
    "# 1c\n",
    "f':bu_business_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_business_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_success_criteria rdfs:label \"1c Business Success Criteria\" .',\n",
    "f':bu_business_success_criteria rdfs:comment \"\"\"{business_success_criteria_comment}\"\"\" .',\n",
    "# 1d\n",
    "f':bu_data_mining_goals rdf:type prov:Entity .',\n",
    "f':bu_data_mining_goals prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_goals rdfs:label \"1d Data Mining Goals\" .',\n",
    "f':bu_data_mining_goals rdfs:comment \"\"\"{data_mining_goals_comment}\"\"\" .',\n",
    "# 1e\n",
    "f':bu_data_mining_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_data_mining_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_success_criteria rdfs:label \"1e Data Mining Success Criteria\" .',\n",
    "f':bu_data_mining_success_criteria rdfs:comment \"\"\"{data_mining_success_criteria_comment}\"\"\" .',\n",
    "# 1f\n",
    "f':bu_ai_risk_aspects rdf:type prov:Entity .',\n",
    "f':bu_ai_risk_aspects prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_ai_risk_aspects rdfs:label \"1f AI risk aspects\" .',\n",
    "f':bu_ai_risk_aspects rdfs:comment \"\"\"{ai_risk_aspects_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(business_understanding_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae9b28",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce717fb",
   "metadata": {},
   "source": [
    "The following pseudo-code & pseudo-documentation may be used as a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':data_understanding_phase rdf:type prov:Activity .',\n",
    "f':data_understanding_phase rdfs:label \"Data Understanding Phase\" .', \n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path  = os.path.join(\"data\", \"datasets\", \"mobile_price\")\n",
    "load_mobile_data_code_writer = student_a\n",
    "def load_mobile_data()-> pd.DataFrame:\n",
    "\n",
    "    ### Load your data\n",
    "    input_file = os.path.join(data_path, 'train.csv')\n",
    "    df = pd.read_csv(input_file)\n",
    "    return df\n",
    "\n",
    "start_time_ld = now()\n",
    "data = load_mobile_data()\n",
    "end_time_ld = now()\n",
    "\n",
    "display(data.head())\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "# Now document the raw data and the loaded data using appropriate ontologies.\n",
    "\n",
    "# Always add these triples for every activity to define the executor!\n",
    "ld_ass_uuid_executor = \"ceb5c10f-749f-486b-b3c0-19ce82a2e393\"  # Generate once\n",
    "load_mobile_data_executor = [\n",
    "    f':load_mobile_data prov:qualifiedAssociation :{ld_ass_uuid_executor} .',\n",
    "    f':{ld_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ld_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(load_mobile_data_executor, prefixes=prefixes)\n",
    "\n",
    "ld_ass_uuid_writer = \"b231c4fc-9670-4126-88aa-2b051acea6ac\"  # Generate once\n",
    "ld_report = \"\"\"\n",
    "Load the mobile price classification training data from train.csv and create\n",
    "a pandas DataFrame with 2000 rows and 21 columns (20 features + 1 target price_range).\n",
    "\"\"\"\n",
    "\n",
    "load_mobile_data_activity = [\n",
    "    # Activity itself\n",
    "    ':load_mobile_data rdf:type prov:Activity .',\n",
    "    ':load_mobile_data sc:isPartOf :data_understanding_phase .',\n",
    "    ':load_mobile_data rdfs:comment \"Data Understanding\" .',\n",
    "    f':load_mobile_data rdfs:comment \"\"\"{ld_report}\"\"\" .',\n",
    "    f':load_mobile_data prov:startedAtTime \"{start_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_mobile_data prov:endedAtTime \"{end_time_ld}\"^^xsd:dateTime .',\n",
    "\n",
    "    # Code writer association\n",
    "    f':load_mobile_data prov:qualifiedAssociation :{ld_ass_uuid_writer} .',\n",
    "    f':{ld_ass_uuid_writer} prov:agent :{load_mobile_data_code_writer} .',\n",
    "    f':{ld_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # INPUT of activity\n",
    "    ':load_mobile_data prov:used :raw_data .',\n",
    "    ':load_mobile_data prov:used :raw_data_path .',\n",
    "    ':raw_data rdf:type prov:Entity .',\n",
    "    ':raw_data_path rdf:type prov:Entity .',\n",
    "    ':raw_data prov:wasDerivedFrom :raw_data_path .',\n",
    "\n",
    "    # OUTPUT of activity\n",
    "    ':data rdf:type prov:Entity .',\n",
    "    ':data prov:wasGeneratedBy :load_mobile_data .',\n",
    "    ':data prov:wasDerivedFrom :raw_data .',\n",
    "]\n",
    "engine.insert(load_mobile_data_activity, prefixes=prefixes)\n",
    "\n",
    "# Further describe the raw data using Croissant\n",
    "raw_data_triples = [\n",
    "    # Dataset level\n",
    "    ':raw_data rdf:type sc:Dataset .',\n",
    "    ':raw_data sc:name \"Mobile Price Classification dataset (raw)\" .',\n",
    "    ':raw_data sc:description \"Kaggle mobile phone specifications with price_range labels (0–3) for 2000 phones.\" .',\n",
    "\n",
    "    # File / distribution\n",
    "    ':mobile_price_csv rdf:type cr:FileObject .',\n",
    "    ':mobile_price_csv sc:name \"train.csv\" .',\n",
    "    ':mobile_price_csv sc:encodingFormat \"text/csv\" .',\n",
    "    ':raw_data sc:distribution :mobile_price_csv .',\n",
    "\n",
    "    # RecordSet describing the tabular structure\n",
    "    ':raw_recordset rdf:type cr:RecordSet .',\n",
    "    ':raw_recordset sc:name \"Table of mobile phone specifications and price range\" .',\n",
    "    ':raw_recordset cr:source :mobile_price_csv .',\n",
    "    ':raw_data cr:recordSet :raw_recordset .',\n",
    "\n",
    "    # === Fields: one entry per column in df.info() ===\n",
    "\n",
    "    # 0 battery_power\n",
    "    ':raw_recordset cr:field :field_battery_power .',\n",
    "    ':field_battery_power rdf:type cr:Field .',\n",
    "    ':field_battery_power sc:name \"battery_power\" .',\n",
    "    ':field_battery_power sc:description \"Total energy the battery can store in one charge (mAh).\" .',\n",
    "    ':field_battery_power cr:dataType xsd:integer .',\n",
    "\n",
    "    # 1 blue\n",
    "    ':raw_recordset cr:field :field_blue .',\n",
    "    ':field_blue rdf:type cr:Field .',\n",
    "    ':field_blue sc:name \"blue\" .',\n",
    "    ':field_blue sc:description \"Binary indicator (0/1) whether the phone has Bluetooth support.\" .',\n",
    "    ':field_blue cr:dataType xsd:integer .',\n",
    "\n",
    "    # 2 clock_speed\n",
    "    ':raw_recordset cr:field :field_clock_speed .',\n",
    "    ':field_clock_speed rdf:type cr:Field .',\n",
    "    ':field_clock_speed sc:name \"clock_speed\" .',\n",
    "    ':field_clock_speed sc:description \"Maximum clock speed of the microprocessor (GHz).\" .',\n",
    "    ':field_clock_speed cr:dataType xsd:double .',\n",
    "\n",
    "    # 3 dual_sim\n",
    "    ':raw_recordset cr:field :field_dual_sim .',\n",
    "    ':field_dual_sim rdf:type cr:Field .',\n",
    "    ':field_dual_sim sc:name \"dual_sim\" .',\n",
    "    ':field_dual_sim sc:description \"Binary indicator (0/1) whether the phone supports dual SIM.\" .',\n",
    "    ':field_dual_sim cr:dataType xsd:integer .',\n",
    "\n",
    "    # 4 fc\n",
    "    ':raw_recordset cr:field :field_fc .',\n",
    "    ':field_fc rdf:type cr:Field .',\n",
    "    ':field_fc sc:name \"fc\" .',\n",
    "    ':field_fc sc:description \"Front camera resolution in megapixels.\" .',\n",
    "    ':field_fc cr:dataType xsd:integer .',\n",
    "\n",
    "    # 5 four_g\n",
    "    ':raw_recordset cr:field :field_four_g .',\n",
    "    ':field_four_g rdf:type cr:Field .',\n",
    "    ':field_four_g sc:name \"four_g\" .',\n",
    "    ':field_four_g sc:description \"Binary indicator (0/1) whether the phone supports 4G.\" .',\n",
    "    ':field_four_g cr:dataType xsd:integer .',\n",
    "\n",
    "    # 6 int_memory\n",
    "    ':raw_recordset cr:field :field_int_memory .',\n",
    "    ':field_int_memory rdf:type cr:Field .',\n",
    "    ':field_int_memory sc:name \"int_memory\" .',\n",
    "    ':field_int_memory sc:description \"Internal memory size of the phone (in GB).\" .',\n",
    "    ':field_int_memory cr:dataType xsd:integer .',\n",
    "\n",
    "    # 7 m_dep\n",
    "    ':raw_recordset cr:field :field_m_dep .',\n",
    "    ':field_m_dep rdf:type cr:Field .',\n",
    "    ':field_m_dep sc:name \"m_dep\" .',\n",
    "    ':field_m_dep sc:description \"Mobile depth (thickness) in cm.\" .',\n",
    "    ':field_m_dep cr:dataType xsd:double .',\n",
    "\n",
    "    # 8 mobile_wt\n",
    "    ':raw_recordset cr:field :field_mobile_wt .',\n",
    "    ':field_mobile_wt rdf:type cr:Field .',\n",
    "    ':field_mobile_wt sc:name \"mobile_wt\" .',\n",
    "    ':field_mobile_wt sc:description \"Weight of the mobile phone in grams.\" .',\n",
    "    ':field_mobile_wt cr:dataType xsd:integer .',\n",
    "\n",
    "    # 9 n_cores\n",
    "    ':raw_recordset cr:field :field_n_cores .',\n",
    "    ':field_n_cores rdf:type cr:Field .',\n",
    "    ':field_n_cores sc:name \"n_cores\" .',\n",
    "    ':field_n_cores sc:description \"Number of cores of the processor (1–8).\" .',\n",
    "    ':field_n_cores cr:dataType xsd:integer .',\n",
    "\n",
    "    # 10 pc\n",
    "    ':raw_recordset cr:field :field_pc .',\n",
    "    ':field_pc rdf:type cr:Field .',\n",
    "    ':field_pc sc:name \"pc\" .',\n",
    "    ':field_pc sc:description \"Primary camera resolution in megapixels.\" .',\n",
    "    ':field_pc cr:dataType xsd:integer .',\n",
    "\n",
    "    # 11 px_height\n",
    "    ':raw_recordset cr:field :field_px_height .',\n",
    "    ':field_px_height rdf:type cr:Field .',\n",
    "    ':field_px_height sc:name \"px_height\" .',\n",
    "    ':field_px_height sc:description \"Pixel resolution height of the mobile display.\" .',\n",
    "    ':field_px_height cr:dataType xsd:integer .',\n",
    "\n",
    "    # 12 px_width\n",
    "    ':raw_recordset cr:field :field_px_width .',\n",
    "    ':field_px_width rdf:type cr:Field .',\n",
    "    ':field_px_width sc:name \"px_width\" .',\n",
    "    ':field_px_width sc:description \"Pixel resolution width of the mobile display.\" .',\n",
    "    ':field_px_width cr:dataType xsd:integer .',\n",
    "\n",
    "    # 13 ram\n",
    "    ':raw_recordset cr:field :field_ram .',\n",
    "    ':field_ram rdf:type cr:Field .',\n",
    "    ':field_ram sc:name \"ram\" .',\n",
    "    ':field_ram sc:description \"Random Access Memory size in MB.\" .',\n",
    "    ':field_ram cr:dataType xsd:integer .',\n",
    "\n",
    "    # 14 sc_h\n",
    "    ':raw_recordset cr:field :field_sc_h .',\n",
    "    ':field_sc_h rdf:type cr:Field .',\n",
    "    ':field_sc_h sc:name \"sc_h\" .',\n",
    "    ':field_sc_h sc:description \"Screen height of the mobile in cm.\" .',\n",
    "    ':field_sc_h cr:dataType xsd:integer .',\n",
    "\n",
    "    # 15 sc_w\n",
    "    ':raw_recordset cr:field :field_sc_w .',\n",
    "    ':field_sc_w rdf:type cr:Field .',\n",
    "    ':field_sc_w sc:name \"sc_w\" .',\n",
    "    ':field_sc_w sc:description \"Screen width of the mobile in cm.\" .',\n",
    "    ':field_sc_w cr:dataType xsd:integer .',\n",
    "\n",
    "    # 16 talk_time\n",
    "    ':raw_recordset cr:field :field_talk_time .',\n",
    "    ':field_talk_time rdf:type cr:Field .',\n",
    "    ':field_talk_time sc:name \"talk_time\" .',\n",
    "    ':field_talk_time sc:description \"Longest time that a single battery charge will last during continuous calls (hours).\" .',\n",
    "    ':field_talk_time cr:dataType xsd:integer .',\n",
    "\n",
    "    # 17 three_g\n",
    "    ':raw_recordset cr:field :field_three_g .',\n",
    "    ':field_three_g rdf:type cr:Field .',\n",
    "    ':field_three_g sc:name \"three_g\" .',\n",
    "    ':field_three_g sc:description \"Binary indicator (0/1) whether the phone supports 3G.\" .',\n",
    "    ':field_three_g cr:dataType xsd:integer .',\n",
    "\n",
    "    # 18 touch_screen\n",
    "    ':raw_recordset cr:field :field_touch_screen .',\n",
    "    ':field_touch_screen rdf:type cr:Field .',\n",
    "    ':field_touch_screen sc:name \"touch_screen\" .',\n",
    "    ':field_touch_screen sc:description \"Binary indicator (0/1) whether the phone has a touch screen.\" .',\n",
    "    ':field_touch_screen cr:dataType xsd:integer .',\n",
    "\n",
    "    # 19 wifi\n",
    "    ':raw_recordset cr:field :field_wifi .',\n",
    "    ':field_wifi rdf:type cr:Field .',\n",
    "    ':field_wifi sc:name \"wifi\" .',\n",
    "    ':field_wifi sc:description \"Binary indicator (0/1) whether the phone supports WiFi.\" .',\n",
    "    ':field_wifi cr:dataType xsd:integer .',\n",
    "\n",
    "    # 20 price_range (target)\n",
    "    ':raw_recordset cr:field :field_price_range .',\n",
    "    ':field_price_range rdf:type cr:Field .',\n",
    "    ':field_price_range sc:name \"price_range\" .',\n",
    "    ':field_price_range sc:description \"Target: price category of the mobile (0=low, 1=medium, 2=high, 3=very high).\" .',\n",
    "    ':field_price_range cr:dataType xsd:integer .',\n",
    "]\n",
    "engine.insert(raw_data_triples, prefixes=prefixes)\n",
    "\n",
    "# Also the output of the load activity is a dataset that can be described with Croissant\n",
    "data_triples = [\n",
    "    ':data rdf:type sc:Dataset .',\n",
    "    ':data sc:name \"Loaded mobile price classification data\" .',\n",
    "    ':data sc:description \"In-memory pandas DataFrame with 2000 rows and 21 columns (20 features + 1 target price_range).\" .',\n",
    "\n",
    "    ':recordset rdf:type cr:RecordSet .',\n",
    "    ':recordset sc:name \"Mobile price DataFrame recordset\" .',\n",
    "    ':data cr:recordSet :recordset .',\n",
    "\n",
    "    # Reuse the same Field individuals for the loaded data\n",
    "    ':recordset cr:field :field_battery_power .',\n",
    "    ':recordset cr:field :field_blue .',\n",
    "    ':recordset cr:field :field_clock_speed .',\n",
    "    ':recordset cr:field :field_dual_sim .',\n",
    "    ':recordset cr:field :field_fc .',\n",
    "    ':recordset cr:field :field_four_g .',\n",
    "    ':recordset cr:field :field_int_memory .',\n",
    "    ':recordset cr:field :field_m_dep .',\n",
    "    ':recordset cr:field :field_mobile_wt .',\n",
    "    ':recordset cr:field :field_n_cores .',\n",
    "    ':recordset cr:field :field_pc .',\n",
    "    ':recordset cr:field :field_px_height .',\n",
    "    ':recordset cr:field :field_px_width .',\n",
    "    ':recordset cr:field :field_ram .',\n",
    "    ':recordset cr:field :field_sc_h .',\n",
    "    ':recordset cr:field :field_sc_w .',\n",
    "    ':recordset cr:field :field_talk_time .',\n",
    "    ':recordset cr:field :field_three_g .',\n",
    "    ':recordset cr:field :field_touch_screen .',\n",
    "    ':recordset cr:field :field_wifi .',\n",
    "    ':recordset cr:field :field_price_range .',\n",
    "]\n",
    "engine.insert(data_triples, prefixes=prefixes)\n",
    "\n",
    "# Also add the units to some fields (example usage of QUDT/SI units)\n",
    "units_triples = [\n",
    "    # Battery power in mAh – treated as a kind of counting/energy-related unit\n",
    "    ':field_battery_power qudt:unit qudt:CountingUnit .',\n",
    "\n",
    "    # RAM and internal memory – information capacity\n",
    "    ':field_ram qudt:unit qudt:InformationUnit .',\n",
    "    ':field_int_memory qudt:unit qudt:InformationUnit .',\n",
    "\n",
    "    # Screen dimensions and depth – lengths in cm\n",
    "    ':field_sc_h qudt:unit siu:centiMeter .',\n",
    "    ':field_sc_w qudt:unit siu:centiMeter .',\n",
    "    ':field_m_dep qudt:unit siu:centiMeter .',\n",
    "\n",
    "    # Weight in grams\n",
    "    ':field_mobile_wt qudt:unit siu:gram .',\n",
    "\n",
    "    # Talk time in hours\n",
    "    ':field_talk_time qudt:unit siu:hour .',\n",
    "\n",
    "    # Pixel resolution – we treat as a counting unit\n",
    "    ':field_px_height qudt:unit qudt:CountingUnit .',\n",
    "    ':field_px_width qudt:unit qudt:CountingUnit .',\n",
    "\n",
    "    # Camera megapixels – also counting-like\n",
    "    ':field_fc qudt:unit qudt:CountingUnit .',\n",
    "    ':field_pc qudt:unit qudt:CountingUnit .',\n",
    "\n",
    "    # Number of cores – plain count\n",
    "    ':field_n_cores qudt:unit qudt:CountingUnit .',\n",
    "\n",
    "    # Binary flags and class label – counts / dimensionless\n",
    "    ':field_blue qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_dual_sim qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_four_g qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_three_g qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_touch_screen qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_wifi qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_price_range qudt:unit qudt:DimensionlessUnit .',\n",
    "]\n",
    "engine.insert(units_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3836382",
   "metadata": {},
   "source": [
    "**Continue with other tasks of the Data Understanding phase such as checking the distribution, skewness, plausibility of values, etc...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Data Understanding – Summary of Variables\n",
    "#############################################\n",
    "\n",
    "du_summary_code_writer = student_a\n",
    "\n",
    "# --- Compute categorical and numerical summaries ---\n",
    "start_time_du = now()\n",
    "\n",
    "df_categorical = data[['price_range', 'n_cores', 'blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']].astype(str)\n",
    "df_numerical = data.drop(columns=df_categorical.columns)\n",
    "\n",
    "categorical_summary = pd.DataFrame({\n",
    "    'Number of Unique Values': df_categorical.nunique(),\n",
    "    'Unique Values': df_categorical.apply(lambda x: x.unique())\n",
    "})\n",
    "\n",
    "numerical_summary = df_numerical.describe().T.round(2)\n",
    "\n",
    "end_time_du = now()\n",
    "\n",
    "display(categorical_summary)\n",
    "display(numerical_summary)\n",
    "\n",
    "#############################################\n",
    "# Provenance Documentation\n",
    "#############################################\n",
    "\n",
    "du_ass_uuid_executor = \"75572bcb-4210-499f-9b34-783657b43f2c\"\n",
    "\n",
    "du_summary_executor = [\n",
    "    f':du_summary prov:qualifiedAssociation :{du_ass_uuid_executor} .',\n",
    "    f':{du_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{du_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{du_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(du_summary_executor, prefixes=prefixes)\n",
    "\n",
    "du_ass_uuid_writer = \"031f9295-0680-462e-8e37-4e096b5512f2\"\n",
    "\n",
    "du_summary_comment = \"\"\"\n",
    "We examined the dataset structure by separating categorical and numerical variables.\n",
    "Categorical features (blue, dual_sim, three_g, four_g, etc.) show mostly binary distributions.\n",
    "The target price_range is evenly distributed across its four classes (0-3), confirming a balanced dataset.\n",
    "Numerical variables (battery_power, ram, px_width, px_height, etc.) show plausible ranges and no missing values.\n",
    "This summary provides an initial understanding of feature types and their variability.\n",
    "\"\"\"\n",
    "\n",
    "du_summary_activity = [\n",
    "    ':du_summary rdf:type prov:Activity .',\n",
    "    ':du_summary sc:isPartOf :data_understanding_phase .',\n",
    "    f':du_summary rdfs:comment \"\"\"{du_summary_comment}\"\"\" .',\n",
    "    f':du_summary prov:startedAtTime \"{start_time_du}\"^^xsd:dateTime .',\n",
    "    f':du_summary prov:endedAtTime \"{end_time_du}\"^^xsd:dateTime .',\n",
    "    f':du_summary prov:qualifiedAssociation :{du_ass_uuid_writer} .',\n",
    "    f':{du_ass_uuid_writer} prov:agent :{du_summary_code_writer} .',\n",
    "    f':{du_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{du_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Outputs\n",
    "    ':categorical_summary rdf:type prov:Entity .',\n",
    "    f':categorical_summary rdfs:comment \"\"\"{categorical_summary.to_string()}\"\"\" .',\n",
    "    ':categorical_summary prov:wasGeneratedBy :du_summary .',\n",
    "\n",
    "    ':numerical_summary rdf:type prov:Entity .',\n",
    "    f':numerical_summary rdfs:comment \"\"\"{numerical_summary.to_string()}\"\"\" .',\n",
    "    ':numerical_summary prov:wasGeneratedBy :du_summary .',\n",
    "]\n",
    "engine.insert(du_summary_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b6269",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Data Understanding – Categorical Distributions\n",
    "#############################################\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import colors\n",
    "\n",
    "catdist_code_writer = student_a\n",
    "start_time_catdist = now()\n",
    "\n",
    "# --- Create Figure ---\n",
    "fig = make_subplots(rows=3, cols=3, specs=[[{'type':'domain'}]*3]*3,\n",
    "                    vertical_spacing=0.05, horizontal_spacing=0.01)\n",
    "\n",
    "for i, feature in enumerate(df_categorical.columns):\n",
    "    value_counts = df_categorical[feature].value_counts()\n",
    "    labels = value_counts.index.tolist()\n",
    "    values = value_counts.values.tolist()\n",
    "  \n",
    "    cmap = colors.LinearSegmentedColormap.from_list(\"purple_contrast\", [\"#6A0DAD\", \"white\"])\n",
    "    \n",
    "    norm = colors.Normalize(vmin=0, vmax=len(labels))\n",
    "    color_list = [colors.rgb2hex(cmap(norm(i))) for i in range(len(labels))]\n",
    "\n",
    "    pie_chart = go.Pie(labels=labels, values=values,\n",
    "                       marker=dict(colors=color_list, line=dict(color='white', width=2)),\n",
    "                       textinfo='percent+label', title=feature)\n",
    "\n",
    "    if i < 8:\n",
    "        fig.add_trace(pie_chart, row=i//3 + 1, col=i%3 + 1)\n",
    "\n",
    "fig.update_layout(title=\"Distribution of Categorical Variables\", height=900, width=900, showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "end_time_catdist = now()\n",
    "\n",
    "#############################################\n",
    "# Provenance Documentation\n",
    "#############################################\n",
    "\n",
    "catdist_ass_uuid_executor = \"f9fb94db-c9c1-42b0-995b-6addac491af3\"\n",
    "\n",
    "catdist_executor = [\n",
    "    f':categorical_distribution prov:qualifiedAssociation :{catdist_ass_uuid_executor} .',\n",
    "    f':{catdist_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{catdist_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{catdist_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(catdist_executor, prefixes=prefixes)\n",
    "\n",
    "catdist_ass_uuid_writer = \"a2007472-d950-437f-8608-9e6ee79b8816\"\n",
    "\n",
    "catdist_comment = \"\"\"\n",
    "Plotted pie-chart distributions for all categorical variables.\n",
    "The price_range classes are evenly distributed (500 samples each).\n",
    "Binary features such as blue, dual_sim, four_g, three_g, wifi, and touch_screen\n",
    "display expected near-even splits, except three_g which shows ~76% phones supporting 3G.\n",
    "These visualizations help assess class balance and detect unusual category frequencies.\n",
    "\"\"\"\n",
    "\n",
    "catdist_activity = [\n",
    "    ':categorical_distribution rdf:type prov:Activity .',\n",
    "    ':categorical_distribution sc:isPartOf :data_understanding_phase .',\n",
    "    f':categorical_distribution rdfs:comment \"\"\"{catdist_comment}\"\"\" .',\n",
    "    f':categorical_distribution prov:startedAtTime \"{start_time_catdist}\"^^xsd:dateTime .',\n",
    "    f':categorical_distribution prov:endedAtTime \"{end_time_catdist}\"^^xsd:dateTime .',\n",
    "    f':categorical_distribution prov:qualifiedAssociation :{catdist_ass_uuid_writer} .',\n",
    "    f':{catdist_ass_uuid_writer} prov:agent :{catdist_code_writer} .',\n",
    "    f':{catdist_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{catdist_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Output entity\n",
    "    ':categorical_distribution_output rdf:type prov:Entity .',\n",
    "    ':categorical_distribution_output rdfs:comment \"Plot showing categorical variable distribution.\" .',\n",
    "    ':categorical_distribution_output prov:wasGeneratedBy :categorical_distribution .'\n",
    "]\n",
    "engine.insert(catdist_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d3f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd368a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Data Understanding – Numerical Distributions & Skewness\n",
    "#############################################\n",
    "\n",
    "numdist_code_writer = student_a\n",
    "start_time_numdist = now()\n",
    "\n",
    "# Create custom colormap\n",
    "cmap = colors.LinearSegmentedColormap.from_list(\"purple_contrast\", [\"#6A0DAD\", \"white\"])\n",
    "\n",
    "# A helper function to generate consistent purple shades\n",
    "def get_color(index, total=20):\n",
    "    norm = colors.Normalize(vmin=0, vmax=total)\n",
    "    return colors.rgb2hex(cmap(norm(index)))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(15,22))\n",
    "\n",
    "skewness_report = {}\n",
    "\n",
    "for i, col in enumerate(df_numerical.columns):\n",
    "    x = i // 3\n",
    "    y = i % 3\n",
    "\n",
    "    # Precompute histogram bin ranges\n",
    "    values, bins = np.histogram(\n",
    "        df_numerical[col],\n",
    "        range=(np.floor(df_numerical[col].min()), np.ceil(df_numerical[col].max()))\n",
    "    )\n",
    "\n",
    "    # Plot histogram\n",
    "    graph = sns.histplot(\n",
    "        df_numerical[col],\n",
    "        bins=bins,\n",
    "        kde=True,\n",
    "        ax=ax[x, y],\n",
    "        color=get_color(i),\n",
    "        alpha=0.7,\n",
    "        edgecolor='white',\n",
    "        line_kws={'lw': 3, 'color': '#6A0DAD'}\n",
    "    )\n",
    "    \n",
    "    # Add count labels on top of the bars\n",
    "    for container in graph.containers:\n",
    "        ax[x, y].bar_label(container, fmt='%d', padding=3, fontsize=8) # fmt='%d' ensures integer counts\n",
    "\n",
    "    ax[x, y].set_xlabel(col, fontsize=13)\n",
    "    ax[x, y].set_ylabel('Count', fontsize=11)\n",
    "    ax[x, y].grid(color='lightgrey')\n",
    "\n",
    "    # Add skewness to report\n",
    "    skewness_report[col] = float(df_numerical[col].skew())\n",
    "\n",
    "# Remove unused axes\n",
    "ax[4, 1].axis('off')\n",
    "ax[4, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Distribution of Numerical Variables', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.show()\n",
    "\n",
    "end_time_numdist = now()\n",
    "\n",
    "#############################################\n",
    "# Provenance Documentation\n",
    "#############################################\n",
    "\n",
    "numdist_ass_uuid_executor = \"b3893fea-29a0-4efb-b0b1-455c93627c9c\"\n",
    "\n",
    "numdist_executor = [\n",
    "    f':numerical_distribution prov:qualifiedAssociation :{numdist_ass_uuid_executor} .',\n",
    "    f':{numdist_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{numdist_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{numdist_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(numdist_executor, prefixes=prefixes)\n",
    "\n",
    "numdist_ass_uuid_writer = \"9cfdac02-de31-4588-819b-76f4ae6a6a5b\"\n",
    "\n",
    "numdist_comment = \"\"\"\n",
    "Histogram analysis shows the distribution of all numerical variables.\n",
    "Most features appear unimodal with realistic ranges. Some variables such as px_height\n",
    "and sc_w contain many values near zero, indicating potential noise.\n",
    "Skewness was computed to identify asymmetry and assess preprocessing needs.\n",
    "\"\"\"\n",
    "\n",
    "numdist_activity = [\n",
    "    ':numerical_distribution rdf:type prov:Activity .',\n",
    "    ':numerical_distribution sc:isPartOf :data_understanding_phase .',\n",
    "    f':numerical_distribution rdfs:comment \"\"\"{numdist_comment}\"\"\" .',\n",
    "    f':numerical_distribution prov:startedAtTime \"{start_time_numdist}\"^^xsd:dateTime .',\n",
    "    f':numerical_distribution prov:endedAtTime \"{end_time_numdist}\"^^xsd:dateTime .',\n",
    "    f':numerical_distribution prov:qualifiedAssociation :{numdist_ass_uuid_writer} .',\n",
    "    f':{numdist_ass_uuid_writer} prov:agent :{numdist_code_writer} .',\n",
    "    f':{numdist_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{numdist_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Output entity with skewness report\n",
    "    ':numerical_skewness_report rdf:type prov:Entity .',\n",
    "    f':numerical_skewness_report rdfs:comment \"\"\"{json.dumps(skewness_report, indent=2)}\"\"\" .',\n",
    "    ':numerical_skewness_report prov:wasGeneratedBy :numerical_distribution .',\n",
    "]\n",
    "engine.insert(numdist_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28fc955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Data Understanding – Outlier Detection (Z-score)\n",
    "#############################################\n",
    "\n",
    "outlier_code_writer = student_a\n",
    "\n",
    "def detect_outliers(df: pd.DataFrame, threshold=3.0):\n",
    "    \"\"\"\n",
    "    Detect outliers in all numeric columns using a z-score threshold.\n",
    "    Returns a dict with count and indices per column.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    df_num = df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "    for col in df_num.columns:\n",
    "        values = df_num[col].astype(float)\n",
    "        mean = values.mean()\n",
    "        std = values.std()\n",
    "\n",
    "        if std == 0 or np.isnan(std):\n",
    "            results[col] = {\"count\": 0, \"indices\": []}\n",
    "            continue\n",
    "\n",
    "        z_scores = (values - mean) / std\n",
    "        mask = np.abs(z_scores) > threshold\n",
    "        outlier_indices = list(values[mask].index)\n",
    "\n",
    "        results[col] = {\n",
    "            \"count\": len(outlier_indices),\n",
    "            \"indices\": outlier_indices\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "start_time_out = now()\n",
    "outlier_report = detect_outliers(data, threshold=3.0)\n",
    "end_time_out = now()\n",
    "\n",
    "print(\"Outlier Detection Report:\")\n",
    "print(json.dumps(outlier_report, indent=2))\n",
    "\n",
    "#############################################\n",
    "# Provenance Documentation\n",
    "#############################################\n",
    "\n",
    "out_ass_uuid_executor = \"0194402e-c08c-4d6f-bf29-3dd0fd549aff\"\n",
    "\n",
    "outlier_executor = [\n",
    "    f':outlier_detection prov:qualifiedAssociation :{out_ass_uuid_executor} .',\n",
    "    f':{out_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{out_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{out_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(outlier_executor, prefixes=prefixes)\n",
    "\n",
    "out_ass_uuid_writer = \"f70ae005-5427-4857-8152-1f9bec395815\"\n",
    "\n",
    "# Short human-readable summary tailored to your actual result\n",
    "outlier_summary_text = \"\"\"\n",
    "Outlier detection was performed using a z-score threshold of 3.0 on all numeric variables.\n",
    "The resulting report shows that no outliers were detected for most features; only 'fc'\n",
    "(front camera megapixels) has 12 observations flagged as potential outliers.\n",
    "All other variables have count = 0 outliers. This suggests the dataset is generally clean,\n",
    "with a small number of unusually high front camera values that can be considered in the\n",
    "data preparation phase.\n",
    "\"\"\"\n",
    "\n",
    "outlier_activity = [\n",
    "    ':outlier_detection rdf:type prov:Activity .',\n",
    "    ':outlier_detection sc:isPartOf :data_understanding_phase .',\n",
    "    f':outlier_detection rdfs:comment \"\"\"{outlier_summary_text}\"\"\" .',\n",
    "    f':outlier_detection prov:startedAtTime \"{start_time_out}\"^^xsd:dateTime .',\n",
    "    f':outlier_detection prov:endedAtTime \"{end_time_out}\"^^xsd:dateTime .',\n",
    "    f':outlier_detection prov:qualifiedAssociation :{out_ass_uuid_writer} .',\n",
    "    f':{out_ass_uuid_writer} prov:agent :{outlier_code_writer} .',\n",
    "    f':{out_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{out_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # JSON report as provenance entity\n",
    "    ':outlier_report rdf:type prov:Entity .',\n",
    "    f':outlier_report rdfs:comment \"\"\"{json.dumps(outlier_report, indent=2)}\"\"\" .',\n",
    "    ':outlier_report prov:wasGeneratedBy :outlier_detection .',\n",
    "]\n",
    "engine.insert(outlier_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701c56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Data Understanding – Correlation Analysis\n",
    "#############################################\n",
    "\n",
    "corr_code_writer = student_a\n",
    "start_time_corr = now()\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = data.corr(numeric_only=True)\n",
    "\n",
    "# Create a mask for the upper triangle (to show only the lower triangle)\n",
    "# mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "\n",
    "# Create purple→white colormap\n",
    "# cmap = colors.LinearSegmentedColormap.from_list(\"purple_contrast\", [\"#6A0DAD\", \"white\"])\n",
    "cmap = colors.LinearSegmentedColormap.from_list(\"purple_contrast_reversed\", [\"white\", \"#6A0DAD\"])\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", annot_kws={\"size\": 8}, mask=mask, cmap=cmap, linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Numerical Variables\", fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "end_time_corr = now()\n",
    "\n",
    "#############################################\n",
    "# Provenance Documentation\n",
    "#############################################\n",
    "\n",
    "corr_ass_uuid_executor = \"0a0709a9-ecaa-4ee3-94c1-2b37b96e1852\"\n",
    "\n",
    "corr_executor = [\n",
    "    f':correlation_analysis prov:qualifiedAssociation :{corr_ass_uuid_executor} .',\n",
    "    f':{corr_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{corr_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{corr_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(corr_executor, prefixes=prefixes)\n",
    "\n",
    "corr_ass_uuid_writer = \"e57283ec-40ef-4c65-8310-87172f233cf3\"\n",
    "\n",
    "corr_comment = \"\"\"\n",
    "A correlation heatmap was computed for all numeric variables including the target price_range.\n",
    "Most off-diagonal entries are dark, indicating generally weak linear correlations between\n",
    "features. A few feature pairs (e.g. front vs. primary camera, screen height vs. screen width,\n",
    "and pixel height vs. pixel width) show slightly higher positive correlations, but no very strong\n",
    "multicollinearity is visible. Correlation between individual features and price_range appears\n",
    "moderate at most. Overall, the heatmap suggests that features provide complementary\n",
    "information without severe redundancy.\n",
    "\"\"\"\n",
    "\n",
    "corr_activity = [\n",
    "    ':correlation_analysis rdf:type prov:Activity .',\n",
    "    ':correlation_analysis sc:isPartOf :data_understanding_phase .',\n",
    "    f':correlation_analysis rdfs:comment \"\"\"{corr_comment}\"\"\" .',\n",
    "    f':correlation_analysis prov:startedAtTime \"{start_time_corr}\"^^xsd:dateTime .',\n",
    "    f':correlation_analysis prov:endedAtTime \"{end_time_corr}\"^^xsd:dateTime .',\n",
    "    f':correlation_analysis prov:qualifiedAssociation :{corr_ass_uuid_writer} .',\n",
    "    f':{corr_ass_uuid_writer} prov:agent :{corr_code_writer} .',\n",
    "    f':{corr_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{corr_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    ':corr_matrix rdf:type prov:Entity .',\n",
    "    f':corr_matrix rdfs:comment \"\"\"{corr_matrix.to_string()}\"\"\" .',\n",
    "    ':corr_matrix prov:wasGeneratedBy :correlation_analysis .',\n",
    "]\n",
    "engine.insert(corr_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01406c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Data Understanding – Data Quality Assessment\n",
    "#############################################\n",
    "\n",
    "dqa_code_writer = student_a\n",
    "start_time_dqa = now()\n",
    "\n",
    "# Programmatic checks\n",
    "missing_values = data.isnull().sum().to_dict()\n",
    "duplicate_count = int(data.duplicated().sum())\n",
    "\n",
    "quality_report = {\n",
    "    \"missing_values\": missing_values,\n",
    "    \"duplicate_rows\": duplicate_count,\n",
    "    \"data_types\": data.dtypes.astype(str).to_dict(),\n",
    "    \"value_range_issues\": {\n",
    "        \"px_height_zero_count\": int((data[\"px_height\"] == 0).sum()),\n",
    "        \"sc_w_zero_count\": int((data[\"sc_w\"] == 0).sum())\n",
    "    },\n",
    "    \"remarks\": (\n",
    "        \"The dataset is complete: all 21 columns have 0 missing values and there are no \"\n",
    "        \"duplicate rows. Numerical data types are consistent with the feature semantics. \"\n",
    "        \"However, px_height has 2 zero values and sc_w has 180 zero values. These zeros are \"\n",
    "        \"unlikely for real screens and may represent noisy or atypical records that should be \"\n",
    "        \"handled carefully in the data preparation phase.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "print(json.dumps(quality_report, indent=2))\n",
    "\n",
    "end_time_dqa = now()\n",
    "\n",
    "#############################################\n",
    "# Provenance Documentation\n",
    "#############################################\n",
    "\n",
    "dqa_ass_uuid_executor = \"27c4dc86-7147-45a9-b17e-ade53867c2e8\"\n",
    "\n",
    "dqa_executor = [\n",
    "    f':data_quality_assessment prov:qualifiedAssociation :{dqa_ass_uuid_executor} .',\n",
    "    f':{dqa_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{dqa_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{dqa_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(dqa_executor, prefixes=prefixes)\n",
    "\n",
    "dqa_ass_uuid_writer = \"5d037fa0-89ed-4ecd-b69b-975d17884658\"\n",
    "\n",
    "dqa_comment = \"\"\"\n",
    "Data quality checks show that the dataset is internally very clean: there are no missing\n",
    "values in any of the 21 columns and no duplicate rows. All features have numeric dtypes\n",
    "consistent with their intended semantics (integers or floats).\n",
    "\n",
    "A potential issue is that px_height contains 2 zero values and sc_w contains 180 zero values.\n",
    "Zero screen height or width is implausible for real mobile devices and may indicate noisy,\n",
    "special, or incorrectly recorded cases. These findings should be considered in the data\n",
    "preparation phase (e.g., deciding whether to filter, impute, or keep these records).\n",
    "\"\"\"\n",
    "\n",
    "dqa_activity = [\n",
    "    ':data_quality_assessment rdf:type prov:Activity .',\n",
    "    ':data_quality_assessment sc:isPartOf :data_understanding_phase .',\n",
    "    f':data_quality_assessment rdfs:comment \"\"\"{dqa_comment}\"\"\" .',\n",
    "    f':data_quality_assessment prov:startedAtTime \"{start_time_dqa}\"^^xsd:dateTime .',\n",
    "    f':data_quality_assessment prov:endedAtTime \"{end_time_dqa}\"^^xsd:dateTime .',\n",
    "    f':data_quality_assessment prov:qualifiedAssociation :{dqa_ass_uuid_writer} .',\n",
    "    f':{dqa_ass_uuid_writer} prov:agent :{dqa_code_writer} .',\n",
    "    f':{dqa_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dqa_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    ':data_quality_report rdf:type prov:Entity .',\n",
    "    f':data_quality_report rdfs:comment \"\"\"{json.dumps(quality_report, indent=2)}\"\"\" .',\n",
    "    ':data_quality_report prov:wasGeneratedBy :data_quality_assessment .',\n",
    "]\n",
    "engine.insert(dqa_activity, prefixes=prefixes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b94dfb7-328c-432b-b7e7-1f66f03eabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Data Understanding – 2e Ethical / Bias Aspects\n",
    "#############################################\n",
    "\n",
    "du_bias_code_writer = student_a\n",
    "start_time_du_bias = now()\n",
    "end_time_du_bias = now()\n",
    "\n",
    "du_bias_ass_uuid_executor = \"a8c4c0ba-0e4a-4b05-9bf7-0c7d7b3e1a10\"\n",
    "\n",
    "du_bias_executor = [\n",
    "    ':du_bias_attributes rdf:type prov:Activity .',\n",
    "    ':du_bias_attributes sc:isPartOf :data_understanding_phase .',\n",
    "    f':du_bias_attributes prov:qualifiedAssociation :{du_bias_ass_uuid_executor} .',\n",
    "    f':{du_bias_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{du_bias_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{du_bias_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    f':du_bias_attributes prov:startedAtTime \"{start_time_du_bias}\"^^xsd:dateTime .',\n",
    "    f':du_bias_attributes prov:endedAtTime \"{end_time_du_bias}\"^^xsd:dateTime .',\n",
    "]\n",
    "engine.insert(du_bias_executor, prefixes=prefixes)\n",
    "\n",
    "du_bias_ass_uuid_writer = \"039cc929-8117-44a3-abcc-f498b4d8d832\"\n",
    "\n",
    "du_bias_comment = \"\"\"\n",
    "The dataset contains no personal or sensitive attributes—only technical phone specifications.\n",
    "The target price_range is perfectly balanced (500 samples per class).\n",
    "Some device types are less frequent (e.g., rare feature combinations), but this affects model\n",
    "performance rather than human fairness. No ethical or demographic bias risks are present.\n",
    "\"\"\"\n",
    "\n",
    "du_bias_activity = [\n",
    "    ':du_bias_attributes_summary rdf:type prov:Entity .',\n",
    "    ':du_bias_attributes_summary prov:wasGeneratedBy :du_bias_attributes .',\n",
    "    ':du_bias_attributes_summary rdfs:label \"2e Ethical / Bias Aspects\" .',\n",
    "    f':du_bias_attributes_summary rdfs:comment \"\"\"{du_bias_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(du_bias_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b4793-5fad-4c9a-89dd-abd662f916b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Data Understanding – 2f Risks & Expert Questions\n",
    "#############################################\n",
    "\n",
    "du_risk_code_writer = student_a\n",
    "start_time_du_risk = now()\n",
    "end_time_du_risk = now()\n",
    "\n",
    "du_risk_ass_uuid_executor = \"1907fabb-d64e-4b65-9a51-b0232c663d03\"\n",
    "\n",
    "du_risk_executor = [\n",
    "    ':du_risk_analysis rdf:type prov:Activity .',\n",
    "    ':du_risk_analysis sc:isPartOf :data_understanding_phase .',\n",
    "    f':du_risk_analysis prov:qualifiedAssociation :{du_risk_ass_uuid_executor} .',\n",
    "    f':{du_risk_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{du_risk_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{du_risk_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    f':du_risk_analysis prov:startedAtTime \"{start_time_du_risk}\"^^xsd:dateTime .',\n",
    "    f':du_risk_analysis prov:endedAtTime \"{end_time_du_risk}\"^^xsd:dateTime .',\n",
    "]\n",
    "engine.insert(du_risk_executor, prefixes=prefixes)\n",
    "\n",
    "du_risk_ass_uuid_writer = \"31229430-b424-43e5-8a2c-39699767bebf\"\n",
    "\n",
    "du_risk_comment = \"\"\"\n",
    "Potential risks include limited representativeness of devices in the dataset, since some \n",
    "feature combinations (e.g., very high camera MP or zero-sized screen dimensions) may not \n",
    "reflect real market distribution. Such records may introduce noise or skew model behavior. \n",
    "There is also a risk that the dataset does not capture newer technologies, causing future \n",
    "model drift.\n",
    "\n",
    "Questions for an external domain expert include:\n",
    "• Are zero values for px_height or sc_w technically valid or measurement artifacts?\n",
    "• Are unusually high fc values real device specifications or outliers?\n",
    "• Does the dataset represent a realistic mix of budget, mid-range, and high-end devices?\n",
    "• Are any important device features missing that strongly influence real-world pricing?\n",
    "\"\"\"\n",
    "\n",
    "du_risk_activity = [\n",
    "    ':du_risk_analysis_summary rdf:type prov:Entity .',\n",
    "    ':du_risk_analysis_summary prov:wasGeneratedBy :du_risk_analysis .',\n",
    "    ':du_risk_analysis_summary rdfs:label \"2f Risks & Expert Questions\" .',\n",
    "    f':du_risk_analysis_summary rdfs:comment \"\"\"{du_risk_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(du_risk_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781e016-c770-43d2-871a-f4f4ab7378b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Data Understanding – 2g Actions Required for Data Preparation\n",
    "#############################################\n",
    "\n",
    "du_actions_code_writer = student_a\n",
    "start_time_du_actions = now()\n",
    "end_time_du_actions = now()\n",
    "\n",
    "du_actions_ass_uuid_executor = \"be62c375-e85e-4104-9863-6a700774143b\"\n",
    "\n",
    "du_actions_executor = [\n",
    "    ':du_preparation_actions rdf:type prov:Activity .',\n",
    "    ':du_preparation_actions sc:isPartOf :data_understanding_phase .',\n",
    "    f':du_preparation_actions prov:qualifiedAssociation :{du_actions_ass_uuid_executor} .',\n",
    "    f':{du_actions_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{du_actions_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{du_actions_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    f':du_preparation_actions prov:startedAtTime \"{start_time_du_actions}\"^^xsd:dateTime .',\n",
    "    f':du_preparation_actions prov:endedAtTime \"{end_time_du_actions}\"^^xsd:dateTime .',\n",
    "]\n",
    "engine.insert(du_actions_executor, prefixes=prefixes)\n",
    "\n",
    "du_actions_ass_uuid_writer = \"b7b77ece-d7e1-4be6-aeae-590c26c65d3a\"\n",
    "\n",
    "du_actions_comment = \"\"\"\n",
    "Based on the data understanding analysis, the following actions are recommended for the\n",
    "data preparation phase:\n",
    "\n",
    "- Handle zero values in px_height (2 cases) and sc_w (180 cases), as such values are \n",
    "  unlikely for real devices; consider filtering or imputing them.\n",
    "\n",
    "- Evaluate whether to treat the 12 outliers in fc (front camera MP) as noise or keep them,\n",
    "  depending on domain expert clarification.\n",
    "\n",
    "- Standardize or scale numerical features (e.g., RAM, battery_power, pixel resolution),\n",
    "  since they vary across different ranges and may affect model performance.\n",
    "\n",
    "- Convert categorical binary features (e.g., blue, dual_sim, four_g, three_g, wifi) to \n",
    "  consistent types if needed, although no encoding is required because they are already numeric.\n",
    "\n",
    "- Review potential skewness in some numerical variables and apply transformations if needed \n",
    "  for algorithms sensitive to non-normality.\n",
    "\n",
    "- Ensure proper train–validation–test splitting to maintain the balanced distribution \n",
    "  of price_range classes.\n",
    "\"\"\"\n",
    "\n",
    "du_actions_activity = [\n",
    "    ':du_preparation_actions_summary rdf:type prov:Entity .',\n",
    "    ':du_preparation_actions_summary prov:wasGeneratedBy :du_preparation_actions .',\n",
    "    ':du_preparation_actions_summary rdfs:label \"2g Actions Required for Data Preparation\" .',\n",
    "    f':du_preparation_actions_summary rdfs:comment \"\"\"{du_actions_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(du_actions_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16349e3",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d290a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Preparation Phase\n",
    "\n",
    "data_preparation_phase_executor = [\n",
    "f':data_preparation_phase rdf:type prov:Activity .',\n",
    "f':data_preparation_phase rdfs:label \"Data Preparation Phase\" .', \n",
    "]\n",
    "engine.insert(data_preparation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7274f7",
   "metadata": {},
   "source": [
    "**Continue with other tasks of the Data Preparation phase such as binning, scaling etc...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Data Preparation – 3a Preprocessing Actions\n",
    "# Duplicate check, missing value check, noise tagging\n",
    "#############################################\n",
    "\n",
    "prep_basic_code_writer = student_b\n",
    "\n",
    "def preprocess_basic(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Perform initial preprocessing steps based on Data Understanding:\n",
    "    - Detect duplicates.\n",
    "    - Detect missing values.\n",
    "    - Detect noise-like values for px_height and sc_w (tag only, do not remove).\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    # Duplicate detection\n",
    "    result[\"duplicate_count\"] = int(df.duplicated().sum())\n",
    "\n",
    "    # Missing values\n",
    "    result[\"missing_values\"] = df.isnull().sum().to_dict()\n",
    "\n",
    "    # Noise detection (according to chosen thresholds)\n",
    "    noise_sc_w = df[df[\"sc_w\"] < 2].index.tolist()\n",
    "    noise_px_height = df[df[\"px_height\"] < 5].index.tolist()\n",
    "\n",
    "    result[\"noise_sc_w_count\"] = len(noise_sc_w)\n",
    "    result[\"noise_px_height_count\"] = len(noise_px_height)\n",
    "    result[\"noise_sc_w_indices\"] = noise_sc_w\n",
    "    result[\"noise_px_height_indices\"] = noise_px_height\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "start_time_prep_basic = now()\n",
    "prep_basic_report = preprocess_basic(data)\n",
    "end_time_prep_basic = now()\n",
    "\n",
    "print(json.dumps(prep_basic_report, indent=2))\n",
    "\n",
    "#############################################\n",
    "# Provenance Documentation\n",
    "#############################################\n",
    "\n",
    "prep_basic_ass_uuid_executor = \"d6e9783f-f036-41c8-8076-400664a30721\"\n",
    "\n",
    "prep_basic_executor = [\n",
    "    f':prep_basic prov:qualifiedAssociation :{prep_basic_ass_uuid_executor} .',\n",
    "    f':{prep_basic_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{prep_basic_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{prep_basic_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(prep_basic_executor, prefixes=prefixes)\n",
    "\n",
    "prep_basic_ass_uuid_writer = \"16e9ce73-e036-4e57-a6be-11d35b0cc868\"\n",
    "\n",
    "prep_basic_comment = \"\"\"\n",
    "Initial preprocessing actions based on the Data Understanding phase:\n",
    "• duplicate_count = 0 → no duplicate rows in the dataset.\n",
    "• All 21 attributes have 0 missing values.\n",
    "• Noise-like values: sc_w < 2 occurs in 390 rows; px_height < 5 occurs in 9 rows.\n",
    "  These cases were identified and recorded but not removed at this stage, because\n",
    "  their interpretation is unclear. We decided to keep them as some models such as SVM are relatively\n",
    "  robust to such noise. Final handling can be decided later based on model behavior.\n",
    "\"\"\"\n",
    "\n",
    "prep_basic_activity = [\n",
    "    ':prep_basic rdf:type prov:Activity .',\n",
    "    ':prep_basic sc:isPartOf :data_preparation_phase .',\n",
    "    f':prep_basic rdfs:comment \"\"\"{prep_basic_comment}\"\"\" .',\n",
    "    f':prep_basic prov:startedAtTime \"{start_time_prep_basic}\"^^xsd:dateTime .',\n",
    "    f':prep_basic prov:endedAtTime \"{end_time_prep_basic}\"^^xsd:dateTime .',\n",
    "    f':prep_basic prov:qualifiedAssociation :{prep_basic_ass_uuid_writer} .',\n",
    "    f':{prep_basic_ass_uuid_writer} prov:agent :{prep_basic_code_writer} .',\n",
    "    f':{prep_basic_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{prep_basic_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Output entity\n",
    "    ':prep_basic_report rdf:type prov:Entity .',\n",
    "    f':prep_basic_report rdfs:comment \"\"\"{json.dumps(prep_basic_report, indent=2)}\"\"\" .',\n",
    "    ':prep_basic_report prov:wasGeneratedBy :prep_basic .',\n",
    "]\n",
    "engine.insert(prep_basic_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a444ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Data Preparation – 3b Steps Considered but Not Applied\n",
    "#############################################\n",
    "\n",
    "prep_considered_code_writer = student_b\n",
    "start_time_prep_considered = now()\n",
    "end_time_prep_considered = now()\n",
    "\n",
    "prep_considered_ass_uuid_executor = \"5edaccd8-5083-4757-ad7a-f79c0ae532af\"\n",
    "\n",
    "prep_considered_executor = [\n",
    "    ':prep_considered rdf:type prov:Activity .',\n",
    "    ':prep_considered sc:isPartOf :data_preparation_phase .',\n",
    "    f':prep_considered prov:qualifiedAssociation :{prep_considered_ass_uuid_executor} .',\n",
    "    f':{prep_considered_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{prep_considered_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{prep_considered_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    f':prep_considered prov:startedAtTime \"{start_time_prep_considered}\"^^xsd:dateTime .',\n",
    "    f':prep_considered prov:endedAtTime \"{end_time_prep_considered}\"^^xsd:dateTime .',\n",
    "]\n",
    "engine.insert(prep_considered_executor, prefixes=prefixes)\n",
    "\n",
    "prep_considered_ass_uuid_writer = \"740fd7bf-4a32-4b46-8f9b-f05a17bb81a7\"\n",
    "\n",
    "prep_considered_comment = \"\"\"\n",
    "Preprocessing steps that were considered during the Data Preparation phase but not applied:\n",
    "\n",
    "• Outlier removal:\n",
    "  Outliers in the front-camera field (fc) were identified earlier. Since their domain\n",
    "  validity is uncertain and they may represent legitimate device variations, they were\n",
    "  not removed at this stage. Their impact will be assessed during modeling.\n",
    "\n",
    "• Noise cleaning:\n",
    "  Values identified as noise-like (sc_w < 2, px_height < 5) were retained for now.\n",
    "  These may correspond to early or atypical devices. Noise handling will be reconsidered\n",
    "  if models show sensitivity to these records.\n",
    "\n",
    "• Feature removal based on low correlation:\n",
    "  Pearson correlation was already evaluated in the Data Understanding phase. Although\n",
    "  some features showed weak linear correlation with the target, correlation alone is not\n",
    "  a reliable criterion for removal, especially when non-linear relationships may exist.\n",
    "  Therefore, no features were dropped solely based on correlation, and model-based\n",
    "  feature importance techniques will be used instead.\n",
    "\n",
    "• Scaling and normalization:\n",
    "  Scaling was considered but postponed. Only algorithms sensitive to feature magnitudes\n",
    "  (e.g., SVM) will require scaling, and it will be applied at the model-specific stage (if required).\n",
    "\n",
    "• Additional encoding:\n",
    "  Binary technical attributes (blue, dual_sim, three_g, four_g, touch_screen, wifi) \n",
    "  are already coded as 0/1. We considered one-hot encoding, but since these are \n",
    "  simple binary flags, no further encoding was applied.\n",
    "\"\"\"\n",
    "\n",
    "prep_considered_activity = [\n",
    "    ':prep_considered_summary rdf:type prov:Entity .',\n",
    "    ':prep_considered_summary prov:wasGeneratedBy :prep_considered .',\n",
    "    ':prep_considered_summary rdfs:label \"3b Preprocessing Steps Considered but Not Applied\" .',\n",
    "    f':prep_considered_summary rdfs:comment \"\"\"{prep_considered_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(prep_considered_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ce36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Data Preparation – 3c Derived Attributes (Options & Potential)\n",
    "#############################################\n",
    "\n",
    "prep_derived_code_writer = student_b\n",
    "start_time_prep_derived = now()\n",
    "end_time_prep_derived = now()\n",
    "\n",
    "prep_derived_ass_uuid_executor = \"88847c41-7e7b-4354-a4e5-40950944c7b0\"\n",
    "\n",
    "prep_derived_executor = [\n",
    "    ':prep_derived_attributes rdf:type prov:Activity .',\n",
    "    ':prep_derived_attributes sc:isPartOf :data_preparation_phase .',\n",
    "    f':prep_derived_attributes prov:qualifiedAssociation :{prep_derived_ass_uuid_executor} .',\n",
    "    f':{prep_derived_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{prep_derived_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{prep_derived_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    f':prep_derived_attributes prov:startedAtTime \"{start_time_prep_derived}\"^^xsd:dateTime .',\n",
    "    f':prep_derived_attributes prov:endedAtTime \"{end_time_prep_derived}\"^^xsd:dateTime .',\n",
    "]\n",
    "engine.insert(prep_derived_executor, prefixes=prefixes)\n",
    "\n",
    "prep_derived_ass_uuid_writer = \"59fb3895-fa42-4b28-bc84-f09abc0cc18f\"\n",
    "\n",
    "prep_derived_comment = \"\"\"\n",
    "Data Preparation 3c - Options and potential for derived attributes:\n",
    "\n",
    "Several derived attributes could potentially improve model performance or interpretability:\n",
    "\n",
    "• Screen-related features:\n",
    "  - pixel_area = px_height * px_width (proxy for screen resolution / sharpness)\n",
    "  - screen_area = sc_h * sc_w (approximate physical display size)\n",
    "  - pixel_density_ratio = pixel_area / screen_area (if sc_w and sc_h are reliable)\n",
    "\n",
    "• Performance / capacity ratios:\n",
    "  - ram_per_internal_memory = ram / int_memory (relative memory configuration)\n",
    "  - battery_per_weight = battery_power / mobile_wt (capacity relative to device weight)\n",
    "  - camera_total_mp = fc + pc (overall camera capability)\n",
    "\n",
    "• Connectivity and feature counts:\n",
    "  - connectivity_score = blue + three_g + four_g + wifi (simple feature count)\n",
    "  - feature_richness = connectivity_score + touch_screen + dual_sim\n",
    "\n",
    "These attributes could better capture interactions between existing variables and might help\n",
    "models distinguish between devices within the same price_range. For now, they are documented\n",
    "as options; actual creation will be decided based on model needs and complexity trade-offs.\n",
    "\"\"\"\n",
    "\n",
    "prep_derived_activity = [\n",
    "    ':prep_derived_attributes_summary rdf:type prov:Entity .',\n",
    "    ':prep_derived_attributes_summary prov:wasGeneratedBy :prep_derived_attributes .',\n",
    "    ':prep_derived_attributes_summary rdfs:label \"3c Derived Attribute Options\" .',\n",
    "    f':prep_derived_attributes_summary rdfs:comment \"\"\"{prep_derived_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(prep_derived_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9ee933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Data Preparation – 3d External Data & Additional Attributes\n",
    "#############################################\n",
    "\n",
    "prep_external_code_writer = student_b\n",
    "start_time_prep_external = now()\n",
    "end_time_prep_external = now()\n",
    "\n",
    "prep_external_ass_uuid_executor = \"3c1838a8-f928-444d-870c-520536819ae2\"\n",
    "\n",
    "prep_external_executor = [\n",
    "    ':prep_external_data rdf:type prov:Activity .',\n",
    "    ':prep_external_data sc:isPartOf :data_preparation_phase .',\n",
    "    f':prep_external_data prov:qualifiedAssociation :{prep_external_ass_uuid_executor} .',\n",
    "    f':{prep_external_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{prep_external_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{prep_external_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    f':prep_external_data prov:startedAtTime \"{start_time_prep_external}\"^^xsd:dateTime .',\n",
    "    f':prep_external_data prov:endedAtTime \"{end_time_prep_external}\"^^xsd:dateTime .',\n",
    "]\n",
    "engine.insert(prep_external_executor, prefixes=prefixes)\n",
    "\n",
    "prep_external_ass_uuid_writer = \"648e4f19-635c-4b92-bbb4-b356285a91d3\"\n",
    "\n",
    "prep_external_comment = \"\"\"\n",
    "Data Preparation 3d - Options for additional external data sources and attributes:\n",
    "\n",
    "The current dataset contains only technical specifications and an abstract price_range label.\n",
    "Several external data sources could improve the alignment with the business goal of realistic\n",
    "pricing and market positioning:\n",
    "\n",
    "• Real retail prices:\n",
    "  Link each device (or representative configurations) to historical market prices from\n",
    "  online shops or price comparison portals. This would allow:\n",
    "  - Training a regression model for actual price\n",
    "  - Validating whether the price_range labels reflect realistic price bands\n",
    "\n",
    "• Brand and model metadata:\n",
    "  Add manufacturer brand, model family, and release year from public product catalogs.\n",
    "  These attributes could capture brand effects and generation effects (newer vs. older\n",
    "  technology) that strongly influence perceived value.\n",
    "\n",
    "• Market segment and region:\n",
    "  If available, add information about target market segment (e.g., budget, mid-range,\n",
    "  flagship) or region (e.g., EU, US, Asia). This would allow more fine-grained analysis\n",
    "  of price expectations across markets.\n",
    "\n",
    "• User / expert ratings:\n",
    "  External quality ratings (camera score, battery score, display rating) aggregated from\n",
    "  review sites could help connect technical specs to perceived quality and justify\n",
    "  differences within the same price_range.\n",
    "\n",
    "These sources are hypothetical and not integrated in this project, but documenting them\n",
    "clarifies how the dataset could be extended to support richer pricing and marketing analysis.\n",
    "\"\"\"\n",
    "\n",
    "prep_external_activity = [\n",
    "    ':prep_external_data_summary rdf:type prov:Entity .',\n",
    "    ':prep_external_data_summary prov:wasGeneratedBy :prep_external_data .',\n",
    "    ':prep_external_data_summary rdfs:label \"3d External Data & Additional Attributes\" .',\n",
    "    f':prep_external_data_summary rdfs:comment \"\"\"{prep_external_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(prep_external_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cedf66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Data Preparation – Summary of Preprocessing Decisions\n",
    "#############################################\n",
    "\n",
    "prep_summary_code_writer = student_b\n",
    "start_time_prep_summary = now()\n",
    "end_time_prep_summary = now()\n",
    "\n",
    "prep_summary_ass_uuid_executor = \"a89dc405-1428-4840-b75f-c25d9f0ceacb\"\n",
    "\n",
    "prep_summary_executor = [\n",
    "    ':prep_summary rdf:type prov:Activity .',\n",
    "    ':prep_summary sc:isPartOf :data_preparation_phase .',\n",
    "    f':prep_summary prov:qualifiedAssociation :{prep_summary_ass_uuid_executor} .',\n",
    "    f':{prep_summary_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{prep_summary_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{prep_summary_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    f':prep_summary prov:startedAtTime \"{start_time_prep_summary}\"^^xsd:dateTime .',\n",
    "    f':prep_summary prov:endedAtTime \"{end_time_prep_summary}\"^^xsd:dateTime .',\n",
    "]\n",
    "engine.insert(prep_summary_executor, prefixes=prefixes)\n",
    "\n",
    "prep_summary_ass_uuid_writer = \"bf36fde6-d754-4ae6-9d0e-5a272074f34a\"\n",
    "\n",
    "prep_summary_comment = \"\"\"\n",
    "Summary of Data Preparation Decisions:\n",
    "\n",
    "Based on the Data Understanding phase, additional preprocessing steps such as global scaling,\n",
    "binning, categorical encoding, and outlier removal were considered but intentionally not applied.\n",
    "\n",
    "• Scaling:\n",
    "  Scaling is not applied globally because only certain models (e.g., SVM) require it. Scaling\n",
    "  will be performed inside model-specific pipelines rather than altering the original dataset.\n",
    "\n",
    "• Binning:\n",
    "  No binning was applied because numerical attributes already show meaningful continuous ranges\n",
    "  and the target price_range classes are balanced. There is no business or modeling justification\n",
    "  for discretization.\n",
    "\n",
    "• Outlier Removal:\n",
    "  Although outliers were detected in the fc feature, they were retained. Their domain validity is\n",
    "  uncertain, and premature removal could discard meaningful variation. These will be revisited\n",
    "  only if models show sensitivity.\n",
    "\n",
    "• Noise Values:\n",
    "  Noise-like values in sc_w and px_height were identified but not cleaned, as these may represent\n",
    "  early-generation devices. Models planned for later (e.g., SVM, Decision Tree, RF) are robust to\n",
    "  such noise.\n",
    "\n",
    "• Encoding:\n",
    "  No encoding was necessary because all categorical features are already numeric (0/1 or ordinal).\n",
    "\n",
    "• Feature Removal:\n",
    "  Although Pearson correlation was analyzed in the Data Understanding phase, features were not\n",
    "  removed solely based on low linear correlation. Non-linear relationships will be captured by\n",
    "  model-based feature importance.\n",
    "\n",
    "Conclusion:\n",
    "Only essential checks (duplicates, missing values, noise tagging) were performed. All other\n",
    "transformations were deferred to the modeling phase or deemed unnecessary given the dataset’s\n",
    "clean and structured nature.\n",
    "\"\"\"\n",
    "\n",
    "prep_summary_activity = [\n",
    "    ':prep_summary_entity rdf:type prov:Entity .',\n",
    "    ':prep_summary_entity rdfs:label \"Data Preparation Summary\" .',\n",
    "    f':prep_summary_entity rdfs:comment \"\"\"{prep_summary_comment}\"\"\" .',\n",
    "    ':prep_summary_entity prov:wasGeneratedBy :prep_summary .',\n",
    "]\n",
    "engine.insert(prep_summary_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0036428-fcdf-4ee8-ad52-424f95024cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your final transformed dataset should also be documented appropriately using Croissant, SI, etc.\n",
    "\n",
    "#############################################\n",
    "# Documentation – Final Prepared Dataset\n",
    "#############################################\n",
    "\n",
    "prepared_data_triples = [\n",
    "    ':prepared_data rdf:type prov:Entity .',\n",
    "    # The prepared dataset is the same as the original loaded dataset,\n",
    "    # because no modifying preprocessing (e.g., removal, imputation) was applied.\n",
    "    ':prepared_data prov:wasDerivedFrom :data .',\n",
    "    ':prepared_data rdf:type sc:Dataset .',\n",
    "    # You may add fields, recordsets, etc., if needed:\n",
    "    # ':prepared_recordset rdf:type cr:RecordSet .',\n",
    "    # ':prepared_data cr:recordSet :prepared_recordset .',\n",
    "    # ':prepared_recordset cr:field :field_x .',\n",
    "]\n",
    "engine.insert(prepared_data_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c19ebb",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb93dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Modeling Phase\n",
    "\n",
    "modeling_phase_executor = [\n",
    "f':modeling_phase rdf:type prov:Activity .',\n",
    "f':modeling rdfs:label \"Modeling Phase\" .', \n",
    "]\n",
    "engine.insert(modeling_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_code_writer = student_a\n",
    "\n",
    "#############################################\n",
    "# Documentation 4a\n",
    "#############################################\n",
    "\n",
    "dma_ass_uuid_writer = \"561e678f-c355-431d-a956-74d642cdd8c4\"\n",
    "dma_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "identify_data_mining_algorithm_activity = [\n",
    "    f':define_algorithm rdf:type prov:Activity .',\n",
    "    f':define_algorithm sc:isPartOf :modeling_phase .',\n",
    "    f':define_algorithm rdfs:comment \"\"\"{dma_comment}\"\"\" .',\n",
    "    f':define_algorithm prov:qualifiedAssociation :{dma_ass_uuid_writer} .',\n",
    "    f':{dma_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{dma_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dma_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # example algorithm definition\n",
    "    f':random_forest_algorithm rdf:type mls:Algorithm .',\n",
    "    f':random_forest_algorithm rdfs:label \"Random Forest Algorithm\" .',\n",
    "\n",
    "    # example implementation\n",
    "    f':random_forrest_classifier_implementation rdf:type mls:Implementation .',\n",
    "    f':random_forrest_classifier_implementation rdfs:label \"Scikit-learn RandomForestClassifier\" .',\n",
    "    f':random_forrest_classifier_implementation mls:implements :random_forest_algorithm .',\n",
    "    f':random_forrest_classifier_implementation prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    \n",
    "    # you can also define your Evaluation Measures here\n",
    "    \n",
    "    # example evaluation \n",
    "    f':r2_score_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':r2_score_measure rdfs:label \"R-squared Score\" .',\n",
    "    f':r2_score_measure rdfs:comment \"xxx\" .',\n",
    "    f':r2_score_measure prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    \n",
    "]\n",
    "engine.insert(identify_data_mining_algorithm_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef613f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation 4b\n",
    "#############################################\n",
    "\n",
    "hp_ass_uuid_writer = \"7e5b5cb5-3755-4a7d-8409-611b11da0bdf\"\n",
    "hp_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "identify_hp_activity = [\n",
    "    f':identify_hyperparameters rdf:type prov:Activity .',\n",
    "    f':identify_hyperparameters sc:isPartOf :modeling_phase .',\n",
    "    f':identify_hyperparameters rdfs:comment \"\"\"{hp_comment}\"\"\" .',\n",
    "    f':identify_hyperparameters prov:qualifiedAssociation :{hp_ass_uuid_writer} .',\n",
    "    f':{hp_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{hp_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{hp_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # example parameter\n",
    "    f':hp_learning_rate rdf:type mls:HyperParameter .',\n",
    "    f':hp_learning_rate rdfs:label \"Learning Rate\" .',\n",
    "    f':hp_learning_rate rdfs:comment \"...\" .',\n",
    "    f':random_forrest_classifier_implementation mls:hasHyperParameter :hp_learning_rate .',\n",
    "    f':hp_learning_rate prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    # continue with your identified hyperparameters\n",
    "    \n",
    "]\n",
    "engine.insert(identify_hp_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995966b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df: pd.DataFrame):\n",
    "    #do something\n",
    "    return 'train_set', 'validation_set', 'test_set'\n",
    "\n",
    "#############################################\n",
    "# Documentation 4c\n",
    "#############################################\n",
    "\n",
    "### Define Train/Validation/Test splits\n",
    "split_ass_uuid_writer = \"2de4d933-67e9-4f29-8db5-500c3e27c782\"\n",
    "split_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "## Use your prepared dataset\n",
    "input_dataset = \":prepared_data\" \n",
    "\n",
    "define_split_activity = [\n",
    "    f':define_data_split rdf:type prov:Activity .',\n",
    "    f':define_data_split sc:isPartOf :modeling_phase .',\n",
    "    f':define_data_split rdfs:comment \"Train/Validation/Test Split Definition\" .',\n",
    "    f':define_data_split rdfs:comment \"\"\"{split_comment}\"\"\" .',\n",
    "    f':define_data_split prov:qualifiedAssociation :{split_ass_uuid_writer} .',\n",
    "    f':{split_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{split_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{split_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':define_data_split prov:used {input_dataset} .',\n",
    "    \n",
    "    # Training Set\n",
    "    f':training_set rdf:type sc:Dataset .',\n",
    "    f':training_set rdfs:label \"Training Set\" .',\n",
    "    f':training_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':training_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':training_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    # Validation Set\n",
    "    f':validation_set rdf:type sc:Dataset .',\n",
    "    f':validation_set rdfs:label \"Validation Set\" .',\n",
    "    f':validation_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':validation_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':validation_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    # Test Set\n",
    "    f':test_set rdf:type sc:Dataset .',\n",
    "    f':test_set rdfs:label \"Test Set\" .',\n",
    "    f':test_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':test_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':test_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    \n",
    "]\n",
    "engine.insert(define_split_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b5ed6-54d6-4c81-9adb-e295fbd5c364",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-978b274ef875c238",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_and_finetune_model(training_set, validation_set):\n",
    "    # do something here\n",
    "\n",
    "    # Try to automate as much documentation work as possible.\n",
    "    # Define your training runs with their respective hyperparameter settings, etc.\n",
    "    # Document each time a training run, model, its hp_settings, evaluations, ...  \n",
    "    # Create performance figures/graphs\n",
    "\n",
    "    return 'Find most suitable model'\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now() \n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4d & e & f\n",
    "#############################################\n",
    "\n",
    "tafm_ass_uuid_writer = \"4b46db41-df33-4105-8596-bf26b3be56c7\"\n",
    "tafm_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "# EXAMPLE output from your training\n",
    "training_run1 = \"run_1\" \n",
    "model_run1 = \"model_run1\"\n",
    "hp1_setting_run1 = \"hp_setting_run1\"\n",
    "eval_train_run1 = \"metric_train_run1\"\n",
    "eval_validation_run1 = \"metric_validation_run1\"\n",
    "\n",
    "\n",
    "train_model_activity = [\n",
    "    # Activity \n",
    "    f':train_and_finetune_model rdf:type prov:Activity .',\n",
    "    f':train_and_finetune_model sc:isPartOf :modeling_phase .',\n",
    "    f':train_and_finetune_model rdfs:comment \"\"\"{tafm_comment}\"\"\" .',\n",
    "    f':train_and_finetune_model prov:startedAtTime \"{start_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:endedAtTime \"{end_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:qualifiedAssociation :{tafm_ass_uuid_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{tafm_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    ########################################\n",
    "    # ONE model run - automate everything below!\n",
    "\n",
    "    # Parameter settings\n",
    "    f':{hp1_setting_run1} rdf:type mls:HyperParameterSetting .',\n",
    "    f':{hp1_setting_run1} mls:specifiedBy :hp_learning_rate .',\n",
    "    f':{hp1_setting_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{hp1_setting_run1} prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "    # add your further parameters\n",
    "\n",
    "    # Describe your Run\n",
    "    f':{training_run1} rdf:type mls:Run .',\n",
    "    f':{training_run1} sc:isPartOf :train_and_finetune_model .',\n",
    "    f':{training_run1} mls:realizes :random_forest_algorithm .',\n",
    "    f':{training_run1} rdf:label \"Training Run 1 with...\" .',\n",
    "    f':{training_run1} mls:executes :your_implementation .', \n",
    "    f':{training_run1} mls:hasInput :training_set .',\n",
    "    f':{training_run1} mls:hasInput :validation_set .',\n",
    "    f':{training_run1} mls:hasInput :{hp1_setting_run1} .',     \n",
    "    # list all your used parameters here\n",
    "    f':{training_run1} mls:hasOutput :{model_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_train_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_validation_run1} .',\n",
    "\n",
    "    # Describe your Model\n",
    "    f':{model_run1} rdf:type mls:Model .',\n",
    "    f':{model_run1} prov:label \"xxx\" .',\n",
    "    f':{model_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{model_run1} mlso:trainedOn :training_set .',\n",
    "    f':{model_run1} mlso:hasAlgorithmType :random_forest_algorithm .',\n",
    "\n",
    "    # Describe your evaluations\n",
    "    # You can have multiple evaluations per model \n",
    "    f':{eval_train_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_train_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_train_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_train_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_train_run1} prov:used :training_set .',\n",
    "\n",
    "    f':{eval_validation_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_validation_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_validation_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_validation_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_validation_run1} prov:used :validation_set .',\n",
    "\n",
    "    # Dont forget to document any visualizations\n",
    "\n",
    "]\n",
    "engine.insert(train_model_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model_full_data(training_set, validation_set):\n",
    "    \n",
    "    # create your\n",
    "    return \"Final Trained Model\"\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now() \n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4g\n",
    "#############################################\n",
    "\n",
    "retrain_ass_uuid_writer = \"19f16231-e2f1-4d97-9e63-2be319d0f66f\" # Generate once\n",
    "\n",
    "final_training_activity = \":retrain_final_model\"\n",
    "final_model = \":final_model_entity\"\n",
    "\n",
    "# Document the retraining activity.\n",
    "# Hint: This activity is still part of the :modeling_phase\n",
    "\n",
    "retrain_documentation = [\n",
    "    # your documentation here    \n",
    "]\n",
    "engine.insert(retrain_documentation, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02059271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06583f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a88bf71f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46137067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Evaluation Phase\n",
    "\n",
    "evaluation_phase_executor = [\n",
    "f':evaluation_phase rdf:type prov:Activity .',\n",
    "f':evaluation_phase rdfs:label \"Evaluation Phase\" .', \n",
    "]\n",
    "engine.insert(evaluation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d80e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_code_writer = student_b\n",
    "def evaluate_on_test_data(final_model, test_set):\n",
    "\n",
    "    # Predict and evaluation on test data\n",
    "        \n",
    "    return 'Performance'\n",
    "\n",
    "start_time_eval = now()\n",
    "#evaluate_on_test_data()\n",
    "end_time_eval = now() \n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "eval_ass_uuid = \"f4e6b5f2-8529-434a-8d83-be38d9ef9ee1\" # Generate once\n",
    "final_model = \":final_model_entity\" \n",
    "test_set = \":test_set\" \n",
    "\n",
    "eval_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "evaluate_activity = [\n",
    "    f':evaluate_final_model rdf:type prov:Activity .',\n",
    "    f':evaluate_final_model sc:isPartOf :evaluation_phase .',\n",
    "    f':evaluate_final_model rdfs:label \"Final Model Evaluation on Test Set\" .',\n",
    "    f':evaluate_final_model rdfs:comment \"\"\"{eval_comment}\"\"\" .',\n",
    "    f':evaluate_final_model prov:startedAtTime \"{start_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:endedAtTime \"{end_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:qualifiedAssociation :{eval_ass_uuid} .',\n",
    "    \n",
    "    f':{eval_ass_uuid} prov:agent :{eval_code_writer} .',\n",
    "    f':{eval_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{eval_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Inputs\n",
    "    f':evaluate_final_model prov:used {final_model} .',\n",
    "    f':evaluate_final_model prov:used {test_set} .',\n",
    "    \n",
    "    # Reference to Data Mining Success Criteria from Phase 1\n",
    "    f':evaluate_final_model prov:used :bu_data_mining_success_criteria .',\n",
    "\n",
    "    # Document you final model performance\n",
    " \n",
    "    # Hint: you evaluate bias in this way:\n",
    "    f':bias_evaluation_result rdf:type mls:ModelEvaluation .',\n",
    "    f':bias_evaluation_result prov:wasGeneratedBy :evaluate_final_model .',\n",
    "    f':bias_evaluation_result rdfs:label \"Bias Analysis\" .',\n",
    "    f':bias_evaluation_result rdfs:comment \"...\" .',\n",
    "    \n",
    "]\n",
    "engine.insert(evaluate_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785c94b",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ad2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Deployment Phase\n",
    "\n",
    "deployment_phase_executor = [\n",
    "f':deployment_phase rdf:type prov:Activity .',\n",
    "f':deployment_phase rdfs:label \"Deployment Phase\" .', \n",
    "]\n",
    "engine.insert(deployment_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "comparison_and_recommendations_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "ethical_aspects_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "monitoring_plan_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "reproducibility_reflection_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "dep_ass_uuid_executor = \"08a1554a-5c38-4eb1-bb25-499b068c14ab\" # Generate once\n",
    "deployment_executor = [\n",
    "f':plan_deployment rdf:type prov:Activity .',\n",
    "f':plan_deployment sc:isPartOf :deployment_phase .', # Connect to Parent Phase\n",
    "f':plan_deployment rdfs:label \"Plan Deployment\"@en .',\n",
    "\n",
    "f':plan_deployment prov:qualifiedAssociation :{dep_ass_uuid_executor} .',\n",
    "f':{dep_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{dep_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{dep_ass_uuid_executor} prov:hadRole :{code_executor_role} .', \n",
    "]\n",
    "engine.insert(deployment_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "deployment_data_executor = [\n",
    "#6a\n",
    "f':dep_recommendations rdf:type prov:Entity .',\n",
    "f':dep_recommendations prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_recommendations rdfs:label \"6a Business Objectives Reflection and Deployment Recommendations\" .',\n",
    "f':dep_recommendations rdfs:comment \"\"\"{comparison_and_recommendations_comment}\"\"\" .',\n",
    "#6b\n",
    "f':dep_ethical_risks rdf:type prov:Entity .',\n",
    "f':dep_ethical_risks prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_ethical_risks rdfs:label \"6b Ethical Aspects and Risks\" .',\n",
    "f':dep_ethical_risks rdfs:comment \"\"\"{ethical_aspects_comment}\"\"\" .',\n",
    "#6c\n",
    "f':dep_monitoring_plan rdf:type prov:Entity .',\n",
    "f':dep_monitoring_plan prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_monitoring_plan rdfs:label \"6c Monitoring Plan\" .',\n",
    "f':dep_monitoring_plan rdfs:comment \"\"\"{monitoring_plan_comment}\"\"\" .',\n",
    "#6d\n",
    "f':dep_reproducibility_reflection rdf:type prov:Entity .',\n",
    "f':dep_reproducibility_reflection prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_reproducibility_reflection rdfs:label \"6d Reproducibility Reflection\" .',\n",
    "f':dep_reproducibility_reflection rdfs:comment \"\"\"{reproducibility_reflection_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(deployment_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e528dac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70d410af",
   "metadata": {},
   "source": [
    "# Generate Latex Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f44e16",
   "metadata": {},
   "source": [
    "The following cells give you an example of how to automatically create a Latex Report from your provenance documentation.\n",
    "\n",
    "Feel free to use the example provided. If you use it, you should adapt and extend it with relevant sections/tables/plots/... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37046b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_iri = f\"https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell includes cleaning functions\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def latex_escape(text: str | None) -> str:\n",
    "    if text is None: return \"\"\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\\", r\"\\textbackslash{}\")\n",
    "    pairs = [\n",
    "        (\"&\", r\"\\&\"), (\"%\", r\"\\%\"), (\"$\", r\"\\$\"), (\"#\", r\"\\#\"), \n",
    "        (\"_\", r\"\\_\"), (\"{\", r\"\\{\"), (\"}\", r\"\\}\"), \n",
    "        (\"~\", r\"\\textasciitilde{}\"), (\"^\", r\"\\textasciicircum{}\")\n",
    "    ]\n",
    "    for k, v in pairs:\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def clean_rdf(x) -> str:\n",
    "    if hasattr(x, \"toPython\"): return str(x.toPython())\n",
    "    if x is None: return \"\"\n",
    "    s = str(x).strip()\n",
    "    s = s.strip('\"').strip(\"'\")\n",
    "    s = s.strip()\n",
    "    if \"^^\" in s:\n",
    "        s = s.split(\"^^\")[0].strip('\"')\n",
    "        \n",
    "    return s\n",
    "\n",
    "def fmt_iso(ts: str) -> str:\n",
    "    if not ts: return \"\"\n",
    "    try:\n",
    "        clean_ts = ts.split(\"^^\")[0].strip('\"')\n",
    "        clean_ts = clean_ts.replace(\"Z\", \"+00:00\") if clean_ts.endswith(\"Z\") else clean_ts\n",
    "        return datetime.fromisoformat(clean_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return latex_escape(str(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell includes exemplary queries for different phases\n",
    "\n",
    "\n",
    "### Author Block\n",
    "author_query = f\"\"\"\n",
    "{prefix_header}\n",
    "PREFIX iao: <http://purl.obolibrary.org/obo/>\n",
    "\n",
    "SELECT DISTINCT ?uri ?given ?family ?matr WHERE {{\n",
    "  VALUES ?uri {{ :{student_a} :{student_b} }}\n",
    "  \n",
    "  ?uri a foaf:Person .\n",
    "  ?uri foaf:givenName ?given .\n",
    "  ?uri foaf:familyName ?family .\n",
    "  ?uri iao:IAO_0000219 ?matr .\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "res_authors = engine.query(author_query)\n",
    "author_block_latex = \"\"\n",
    "\n",
    "if not res_authors.empty: # type:ignore\n",
    "    for _, row in res_authors.iterrows(): # type:ignore\n",
    "\n",
    "        uri_str = str(row['uri'])\n",
    "        given = latex_escape(clean_rdf(row['given']))\n",
    "        family = latex_escape(clean_rdf(row['family']))\n",
    "        matr = latex_escape(clean_rdf(row['matr']))\n",
    "        if student_a in uri_str:\n",
    "            responsibility = \"Student A\"\n",
    "        elif student_b in uri_str:\n",
    "            responsibility = \"Student B\"\n",
    "        else:\n",
    "            responsibility = \"Student\"\n",
    "        \n",
    "        author_block_latex += rf\"\"\"\n",
    "          \\author{{{given} {family}}}\n",
    "          \\authornote{{{responsibility}, Matr.Nr.: {matr}}}\n",
    "          \\affiliation{{\n",
    "            \\institution{{TU Wien}}\n",
    "            \\country{{Austria}}\n",
    "          }}\n",
    "          \"\"\"\n",
    "\n",
    "### Business Understanding example\n",
    "bu_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?ds_comment ?bo_comment WHERE {{\n",
    "  OPTIONAL {{ :bu_data_source_and_scenario rdfs:comment ?ds_comment . }}\n",
    "  OPTIONAL {{ :bu_business_objectives rdfs:comment ?bo_comment . }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_bu = engine.query(bu_query)\n",
    "row_bu = res_bu.iloc[0] if not res_bu.empty else {} # type:ignore\n",
    "bu_data_source = latex_escape(clean_rdf(row_bu.get(\"ds_comment\", \"\")))\n",
    "bu_objectives  = latex_escape(clean_rdf(row_bu.get(\"bo_comment\", \"\")))\n",
    "\n",
    "\n",
    "### Data Understanding examples\n",
    "# Example Dataset Description\n",
    "du_desc_query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?desc WHERE {{ :raw_data sc:description ?desc . }} LIMIT 1\n",
    "\"\"\"\n",
    "res_du_desc = engine.query(du_desc_query)\n",
    "row_du_desc = res_du_desc.iloc[0] if not res_du_desc.empty else {} # type:ignore\n",
    "du_description = latex_escape(clean_rdf(row_du_desc.get(\"desc\", \"\")))\n",
    "\n",
    "# Example Feature Columns Table\n",
    "du_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?name (SAMPLE(?dtypeRaw) as ?dtype) (SAMPLE(?descRaw) as ?desc) WHERE {{\n",
    "  :raw_data cr:recordSet ?rs .\n",
    "  ?rs cr:field ?field .\n",
    "  ?field sc:name ?name .\n",
    "  ?field sc:description ?descRaw .\n",
    "  ?field cr:dataType ?dtypeRaw .\n",
    "}} \n",
    "GROUP BY ?name\n",
    "ORDER BY ?name\n",
    "\"\"\"\n",
    "res_du = engine.query(du_query)\n",
    "du_rows = []\n",
    "if not res_du.empty: # type:ignore\n",
    "    for _, f in res_du.iterrows(): # type:ignore\n",
    "        dtype_raw = clean_rdf(f.get(\"dtype\", \"\"))\n",
    "        if '#' in dtype_raw: dtype = dtype_raw.split('#')[-1]\n",
    "        elif '/' in dtype_raw: dtype = dtype_raw.split('/')[-1]\n",
    "        else: dtype = dtype_raw\n",
    "        \n",
    "        desc = clean_rdf(f.get(\"desc\", \"\"))\n",
    "        row_str = f\"{latex_escape(clean_rdf(f['name']))} & {latex_escape(dtype)} & {latex_escape(desc)} \\\\\\\\\"\n",
    "        du_rows.append(row_str)\n",
    "du_table_rows = \"\\n    \".join(du_rows)\n",
    "\n",
    "### Modeling example\n",
    "# Hyperparameters\n",
    "hp_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?hpName (SAMPLE(?hpValRaw) as ?hpVal) (MAX(?hpDescRaw) as ?hpDesc) WHERE {{\n",
    "  ?run sc:isPartOf :train_and_finetune_model .\n",
    "  ?run mls:hasInput ?setting .\n",
    "  ?setting a mls:HyperParameterSetting .\n",
    "  ?setting mls:hasValue ?hpValRaw .\n",
    "  ?setting mls:specifiedBy ?hpDef .\n",
    "  ?hpDef rdfs:label ?hpName .\n",
    "  OPTIONAL {{ ?hpDef rdfs:comment ?hpDescRaw . }}\n",
    "}} \n",
    "GROUP BY ?hpName\n",
    "ORDER BY ?hpName\n",
    "\"\"\"\n",
    "res_hp = engine.query(hp_query)\n",
    "hp_rows = []\n",
    "if not res_hp.empty: #type:ignore\n",
    "    for _, row in res_hp.iterrows(): #type:ignore\n",
    "        name = latex_escape(clean_rdf(row['hpName']))\n",
    "        val  = latex_escape(clean_rdf(row['hpVal']))\n",
    "        desc = latex_escape(clean_rdf(row.get('hpDesc', '')))\n",
    "        hp_rows.append(rf\"{name} & {desc} & {val} \\\\\")\n",
    "\n",
    "hp_table_rows = \"\\n    \".join(hp_rows)\n",
    "\n",
    "# Run Info\n",
    "run_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?algoLabel ?start ?end ?metricLabel ?metricVal WHERE {{\n",
    "  OPTIONAL {{ :train_and_finetune_model prov:startedAtTime ?start ; prov:endedAtTime ?end . }}\n",
    "  OPTIONAL {{\n",
    "      ?run sc:isPartOf :train_and_finetune_model .\n",
    "      ?run mls:realizes ?algo .\n",
    "      ?algo rdfs:label ?algoLabel .\n",
    "  }}\n",
    "  OPTIONAL {{\n",
    "    ?run sc:isPartOf :train_and_finetune_model .\n",
    "    ?run mls:hasOutput ?eval .\n",
    "    ?eval a mls:ModelEvaluation ; mls:hasValue ?metricVal .\n",
    "    OPTIONAL {{ ?eval mls:specifiedBy ?m . ?m rdfs:label ?metricLabel . }}\n",
    "  }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_run = engine.query(run_query)\n",
    "row_run = res_run.iloc[0] if not res_run.empty else {} #type:ignore\n",
    "mod_algo  = latex_escape(clean_rdf(row_run.get(\"algoLabel\", \"\")))\n",
    "mod_start = latex_escape(fmt_iso(clean_rdf(row_run.get(\"start\"))))\n",
    "mod_end   = latex_escape(fmt_iso(clean_rdf(row_run.get(\"end\"))))\n",
    "mod_m_lbl = latex_escape(clean_rdf(row_run.get(\"metricLabel\", \"\")))\n",
    "raw_val = clean_rdf(row_run.get('metricVal', ''))\n",
    "mod_m_val = f\"{float(raw_val):.4f}\" if raw_val else \"\"\n",
    "\n",
    "print(\"Data extraction done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8fa1c",
   "metadata": {},
   "source": [
    "The following includes the Latex report itself. It fills in the query-results from the cell before. The ACM Template is already filled. \n",
    "Make sure that you update Student A and B accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ce52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_content = rf\"\"\"\\documentclass[sigconf]{{acmart}}\n",
    "\n",
    "\\AtBeginDocument{{ \\providecommand\\BibTeX{{ Bib\\TeX }} }}\n",
    "\\setcopyright{{acmlicensed}}\n",
    "\\copyrightyear{{2025}}\n",
    "\\acmYear{{2025}}\n",
    "\\acmDOI{{XXXXXXX.XXXXXXX}}\n",
    "\n",
    "\\acmConference[BI 2025]{{Business Intelligence}}{{-}}{{-}}\n",
    "\n",
    "\\begin{{document}}\n",
    "\n",
    "\\title{{BI2025 Experiment Report - Group {group_id}}}\n",
    "%% ---Authors: Dynamically added ---\n",
    "{author_block_latex}\n",
    "\n",
    "\\begin{{abstract}}\n",
    "  This report documents the machine learning experiment for Group {group_id}, following the CRISP-DM process model.\n",
    "\\end{{abstract}}\n",
    "\n",
    "\\ccsdesc[500]{{Computing methodologies~Machine learning}}\n",
    "\\keywords{{CRISP-DM, Provenance, Knowledge Graph, Machine Learning}}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "%% --- 1. Business Understanding ---\n",
    "\\section{{Business Understanding}}\n",
    "\n",
    "\\subsection{{Data Source and Scenario}}\n",
    "{bu_data_source}\n",
    "\n",
    "\\subsection{{Business Objectives}}\n",
    "{bu_objectives}\n",
    "\n",
    "%% --- 2. Data Understanding ---\n",
    "\\section{{Data Understanding}}\n",
    "\\textbf{{Dataset Description:}} {du_description}\n",
    "\n",
    "The following features were identified in the dataset:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Raw Data Features}}\n",
    "  \\label{{tab:features}}\n",
    "  \\begin{{tabular}}{{lp{{0.2\\linewidth}}p{{0.4\\linewidth}}}}\n",
    "    \\toprule\n",
    "    \\textbf{{Feature Name}} & \\textbf{{Data Type}} & \\textbf{{Description}} \\\\\n",
    "    \\midrule\n",
    "    {du_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "%% --- 3. Data Preparation ---\n",
    "\\section{{Data Preparation}}\n",
    "\\subsection{{Data Cleaning}}\n",
    "Describe your Data preparation steps here and include respective graph data.\n",
    "\n",
    "\n",
    "%% --- 4. Modeling ---\n",
    "\\section{{Modeling}}\n",
    "\n",
    "\\subsection{{Hyperparameter Configuration}}\n",
    "The model was trained using the following hyperparameter settings:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Hyperparameter Settings}}\n",
    "  \\label{{tab:hyperparams}}\n",
    "  \\begin{{tabular}}{{lp{{0.4\\linewidth}}l}}\n",
    "    \\toprule\n",
    "    \\textbf{{Parameter}} & \\textbf{{Description}} & \\textbf{{Value}} \\\\\n",
    "    \\midrule\n",
    "    {hp_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "\\subsection{{Training Run}}\n",
    "A training run was executed with the following characteristics:\n",
    "\\begin{{itemize}}\n",
    "    \\item \\textbf{{Algorithm:}} {mod_algo}\n",
    "    \\item \\textbf{{Start Time:}} {mod_start}\n",
    "    \\item \\textbf{{End Time:}} {mod_end}\n",
    "    \\item \\textbf{{Result:}} {mod_m_lbl} = {mod_m_val}\n",
    "\\end{{itemize}}\n",
    "\n",
    "%% --- 5. Evaluation ---\n",
    "\\section{{Evaluation}}\n",
    "\n",
    "%% --- 6. Deployment ---\n",
    "\\section{{Deployment}}\n",
    "\n",
    "\\section{{Conclusion}}\n",
    "\n",
    "\\end{{document}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c947b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell stores the Latex report to the data/report directory\n",
    "\n",
    "out_dir = os.path.join(\"data\", \"report\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"experiment_report.tex\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_content)\n",
    "\n",
    "print(f\"Report written to: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BI2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
